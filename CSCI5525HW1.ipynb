{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSCI5525HW1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN8+TyW+eKA14TIrBlcjs+5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TonyLiu836/CSCI5525-MachineLearning/blob/main/CSCI5525HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CSCI5525 HW 1\n",
        "Tony Liu - ID: 5383942"
      ],
      "metadata": {
        "id": "6nyGyljKlWTh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyGXGnjGlTcT",
        "outputId": "41063373-fd83-4eda-a6cf-0e33adc04a73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets \n",
        "import math\n",
        "\n",
        "#change path for datasets\n",
        "\n",
        "boston_data_path = \"/content/drive/MyDrive/Grad School/Spring2022/CSCI5525 ML/HW1/boston.csv\"        \n",
        "digits_data_path = \"/content/drive/MyDrive/Grad School/Spring2022/CSCI5525 ML/HW1/digits.csv\"\n",
        "#boston_data_path = \"C:\\\\GradSchool\\\\Spring2022\\\\CSCI5525ML\\\\HW1\\\\boston.csv\"\n",
        "#digits_data_path = \"C:\\\\GradSchool\\\\Spring2022\\\\CSCI5525ML\\\\HW1\\\\digits.csv\"\n"
      ],
      "metadata": {
        "id": "M2WogBR5L16O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def getBostonDataset(tau):\n",
        "    boston = np.loadtxt(open(boston_data_path, \"rb\"), delimiter=\",\", skiprows=1)\n",
        "    boston = np.asarray(boston)\n",
        "    #X = boston[:,:-1]\n",
        "    y = boston[:,-1:]\n",
        "    boundary_value = np.percentile(y, tau)      #split boston dataset into either 50 or 75 \n",
        "    for i in range(len(y)):\n",
        "        if y[i] >= boundary_value:\n",
        "            boston[i][-1] = 1\n",
        "        else:\n",
        "            boston[i][-1] = 0\n",
        "    #return X, y\n",
        "    return boston\n",
        "\n",
        "def getDigitsDataset():\n",
        "    digits = np.loadtxt(open(digits_data_path, \"rb\"), delimiter=\",\", skiprows=1)\n",
        "    #data = datasets.load_digits()\n",
        "    return digits\n",
        "\n",
        "\n",
        "def chooseDataset(dataset):\n",
        "    if dataset == \"Boston50\":\n",
        "        boston50 = getBostonDataset(50)\n",
        "        return boston50\n",
        "\n",
        "    elif dataset == \"Boston75\":\n",
        "        boston75= getBostonDataset(75)\n",
        "        return boston75\n",
        "\n",
        "    else:\n",
        "        digits = getDigitsDataset()\n",
        "        return digits\n",
        "\n"
      ],
      "metadata": {
        "id": "U20sjL1YmEjG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA1dThres (Working)"
      ],
      "metadata": {
        "id": "hvH6TaH4J1XD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randrange\n",
        "from collections import defaultdict\n",
        "#split data into 10-folds\n",
        "def crossValSplit(dataset, folds = 10):\n",
        "    fold_size = int(len(dataset) / folds)\n",
        "    split_dataset = []\n",
        "    dataset_copy = dataset\n",
        "    for i in range(folds):\n",
        "        each_fold = []\n",
        "        while len(each_fold) < fold_size:\n",
        "            randSample = randrange(len(dataset_copy[0]))\n",
        "\n",
        "            each_fold.append(dataset_copy[randSample])\n",
        "            np.delete(dataset_copy,randSample)\n",
        "        split_dataset.append(each_fold)\n",
        "    \n",
        "    return split_dataset \n",
        "\n",
        "#seperate samples into classes based on their labels\n",
        "def seperateData(data, labels):\n",
        "    class1 = []       #y=1 class\n",
        "    class0 = []       #y=0 class\n",
        "    for i in range(np.shape(data)[0]):\n",
        "\n",
        "        if labels[i] == 1:\n",
        "            class1.append(data[i])\n",
        "        else:\n",
        "            class0.append(data[i])\n",
        "    return class0, class1\n",
        "\n",
        "def softmax(vect):\n",
        "    e = np.exp(vect)\n",
        "    return e / e.sum()\n",
        "\n",
        "\n",
        "#get mean of features in each class\n",
        "def calcMeans(class0, class1):\n",
        "    #print(np.shape(class0))\n",
        "    class1Mean = np.mean(class1, axis = 0)      \n",
        "    class0Mean = np.mean(class0, axis = 0)      \n",
        "    return class0Mean, class1Mean\n",
        "\n",
        "#finds the between-class scatter(SB) and within-class scatter(SW)\n",
        "def calcCovariances(class0, class1, class0Mean, class1Mean):\n",
        "    diffMeans = np.asarray(class1Mean - class0Mean)\n",
        "    SB = np.matmul(np.transpose([diffMeans]), [diffMeans])\n",
        "    length = len(class0[0])\n",
        "    sum0 = 0\n",
        "    sum1 = 0\n",
        "    for n in range(len(class0Mean)):\n",
        "        temp0 = class0[n] - class0Mean\n",
        "        sum0 += np.matmul(np.transpose([temp0]), [temp0])\n",
        "\n",
        "    for n in range(len(class1Mean)):  \n",
        "        temp1 = class1[n] - class1Mean\n",
        "        sum1 += np.matmul(np.transpose([temp1]), [temp1])\n",
        "\n",
        "    SW = sum0 + sum1\n",
        "\n",
        "    return SB, SW\n",
        "\n",
        "\n",
        "# projects samples data @ W.T\n",
        "def projectData(W, data):\n",
        "    #projectedPts = []\n",
        "    #print(np.shape(data))\n",
        "    #print(np.shape(W))\n",
        "    projectedPts = data @ W\n",
        "\n",
        "    return projectedPts\n",
        "\n",
        "\n",
        "\n",
        "# takes in samples of both classes and then finds the best boundary to classify samples\n",
        "def trainLDA1D(train_data, train_labels):\n",
        "    class0, class1 = seperateData(train_data,train_labels)\n",
        "    class0Mean, class1Mean = calcMeans(class0, class1)\n",
        "    SB,SW = calcCovariances(class0,class1, class0Mean, class1Mean)\n",
        "\n",
        "    #eigVal, eigVect = np.linalg.eig(np.matmul(np.linalg.pinv(SW),SB))     #find the largest eigenvalue and the corresponding eigenvector\n",
        "    #largestEigValInd = np.where(eigVal == max(eigVal))\n",
        "    #W = eigVect[largestEigValInd][0]                                    #set W to be the previously found eigenvector\n",
        "\n",
        "    eigVal, eigVect = np.linalg.eig(np.linalg.pinv(SW) @ SB)\n",
        "    eigPairs = [(abs(eigVal[i]), eigVect[:,i]) for i in range(len(eigVal))]       #combine the eigval & eigvect into pairs\n",
        "    sortedEigPairs = sorted(eigPairs, key = lambda x: x[0], reverse = True)       #sort the eigen pairs in descending order \n",
        "    W = np.transpose([sortedEigPairs[0][1]])\n",
        "    #print(W)\n",
        "  \n",
        "    projPtsC0 = projectData(W, class0)\n",
        "    projPtsC1 = projectData(W, class1)\n",
        "    \n",
        "    projC0Mean, projC1Mean = calcMeans(projPtsC0, projPtsC1)            #assume P(x|Ci) are Gaussian and calc mean, cov, priors\n",
        "    projMeans = [projC0Mean, projC1Mean]\n",
        "\n",
        "    projC0Cov = np.cov(projPtsC0, rowvar = False)\n",
        "    projC1Cov = np.cov(projPtsC1, rowvar = False)\n",
        "    projCovs = [projC0Cov, projC1Cov]\n",
        "\n",
        "    priorC1 = len(class0)/(len(class0) + len(class1))\n",
        "    priorC2 = len(class1)/ (len(class0) + len(class1))\n",
        "    priors = [priorC1, priorC2]\n",
        "\n",
        "    return W, projMeans, projCovs, priors\n",
        "\n",
        "def predict(data, W, means, covs, priors):\n",
        "    y_hat = []\n",
        "    proj = projectData(W, data)  \n",
        "    proj = np.asarray(proj)\n",
        "    for sample in proj:\n",
        "        sample_pred = []\n",
        "        for i in range(len(means)):                     #use the un-simplified discriminant form \n",
        "            #invCov = np.linalg.inv(covs[i])\n",
        "            t1 = -(0.5) * np.log(2*math.pi)\n",
        "            t2 = np.log(covs[i])\n",
        "            t3 = ((sample - means[i])**2) / (2*covs[i]**2)\n",
        "            t4 = np.log(priors[i])\n",
        "            g = t1 - t2 - t3 + t4\n",
        "            sample_pred.append(g)\n",
        "        pred = softmax(sample_pred)\n",
        "        y_hat.append(np.argmax(pred))\n",
        "    \n",
        "    return y_hat\n",
        "\n",
        "\n",
        "def errorAndStdDev(prediction, gndTruth):\n",
        "    counter = 0\n",
        "    temp = 0\n",
        "    gndTruthMean = sum(gndTruth) / len(gndTruth)\n",
        "    for i in range(len(prediction)):\n",
        "        #print(prediction[i], gndTruth[i])\n",
        "        if prediction[i] != gndTruth[i]:\n",
        "            counter += 1\n",
        "    \n",
        "    error = counter / len(prediction) * 100\n",
        "    \n",
        "    mean = np.mean(gndTruth)\n",
        "    for i in range(len(prediction)):\n",
        "        temp += (prediction[i] - mean)**2\n",
        "    stdDev = np.sqrt(temp/len(gndTruth)) / np.sqrt(len(prediction)) *100 \n",
        "    return error, stdDev\n",
        "\n",
        "def LDA1dThres(filename, num_crossval):#, prior):\n",
        "    split_data = crossValSplit(filename)\n",
        "    \n",
        "    globalError = 0\n",
        "    globalStdDev = 0\n",
        "\n",
        "    fold_error = []\n",
        "    fold_stddev = []\n",
        "    for i in range(len(split_data)):\n",
        "        #print(len(split_data[i]))\n",
        "        test_data = np.copy(split_data[i])\n",
        "        test_labels = test_data[:,-1]\n",
        "        test_data = test_data[:,:-1]\n",
        "\n",
        "        train_data_folds = np.delete(np.copy(split_data),i,0)\n",
        "        folds, samples, features = np.shape(train_data_folds)\n",
        "        train_data = np.reshape(train_data_folds, (folds * samples, features))\n",
        "        \n",
        "        train_labels = train_data[:,-1]\n",
        "\n",
        "        train_data = train_data[:,:-1]\n",
        "\n",
        "        class0, class1 = seperateData(train_data, train_labels)\n",
        "        W, means,covs, priors = trainLDA1D(train_data, train_labels)\n",
        "        trainPred = predict(train_data, W, means, covs, priors)\n",
        "        trainError, trainStdDev = errorAndStdDev(trainPred, train_labels)\n",
        "\n",
        "        prediction = predict(test_data, W, means, covs, priors)\n",
        "        error, stdDev= errorAndStdDev(prediction, test_labels)\n",
        "        #fold_error.append(error)\n",
        "        #fold_stddev.append(stdDev)\n",
        "        globalStdDev += stdDev\n",
        "        globalError += error\n",
        "        print(\"Fold= \", i, \"  Train error= \", trainError, \"%  Train std dev= \", trainStdDev, \"  Test Error\", error, \"%  Test std dev= \", stdDev)\n",
        "    \n",
        "    \n",
        "    print(\"Avg Error = \", globalError / num_crossval, \"%\")\n",
        "    print(\"Avg Standard Deviation = \", globalStdDev / num_crossval)\n",
        "\n",
        "#dataset, prior = chooseDataset(\"Boston50\")\n",
        "#print(\"dataset=\", dataset)\n",
        "dataset= chooseDataset(\"Boston50\")\n",
        "LDA1dThres(dataset, 10)#, prior)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_H5mfK16iOt",
        "outputId": "f72714f4-0b0f-47c9-849e-92fd7ad2a126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in true_divide\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold=  0   Train error=  26.666666666666668 %  Train std dev=  2.434322477800744   Test Error 22.0 %  Test std dev=  7.0710678118654755\n",
            "Fold=  1   Train error=  33.77777777777778 %  Train std dev=  2.418039201552962   Test Error 48.0 %  Test std dev=  8.376156636548767\n",
            "Fold=  2   Train error=  28.000000000000004 %  Train std dev=  2.4179484326120124   Test Error 30.0 %  Test std dev=  7.63675323681471\n",
            "Fold=  3   Train error=  27.555555555555557 %  Train std dev=  2.432089991837274   Test Error 28.000000000000004 %  Test std dev=  7.222188034107117\n",
            "Fold=  4   Train error=  28.666666666666668 %  Train std dev=  2.4352464433922987   Test Error 24.0 %  Test std dev=  7.200000000000002\n",
            "Fold=  5   Train error=  28.22222222222222 %  Train std dev=  2.4390512896238294   Test Error 22.0 %  Test std dev=  7.0710678118654755\n",
            "Fold=  6   Train error=  27.555555555555557 %  Train std dev=  2.4270989530396907   Test Error 28.000000000000004 %  Test std dev=  7.359347797189642\n",
            "Fold=  7   Train error=  27.111111111111114 %  Train std dev=  2.4356745037188103   Test Error 32.0 %  Test std dev=  7.0710678118654755\n",
            "Fold=  8   Train error=  28.666666666666668 %  Train std dev=  2.4404006956964226   Test Error 24.0 %  Test std dev=  7.0710678118654755\n",
            "Fold=  9   Train error=  28.000000000000004 %  Train std dev=  2.4333530637905705   Test Error 30.0 %  Test std dev=  7.233256528010052\n",
            "Avg Error =  28.8 %\n",
            "Avg Standard Deviation =  7.331197348013218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA2dGaussianGM (Working)"
      ],
      "metadata": {
        "id": "pfTwRvgYJOY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randrange\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#split data into 10-folds\n",
        "def crossValSplit(dataset, folds = 10):\n",
        "    fold_size = int(len(dataset) / folds)\n",
        "    split_dataset = []\n",
        "    dataset_copy = dataset\n",
        "    for i in range(folds):\n",
        "        each_fold = []\n",
        "        while len(each_fold) < fold_size:\n",
        "            randSample = randrange(len(dataset_copy[0]))\n",
        "            each_fold.append(dataset_copy[randSample])\n",
        "            np.delete(dataset_copy,randSample)\n",
        "        split_dataset.append(each_fold)\n",
        "    \n",
        "    return split_dataset\n",
        "    \n",
        "def errorAndStdDev(y_hat, gndTruth):\n",
        "    counter = 0\n",
        "    temp = 0\n",
        "    gndTruthMean = sum(gndTruth) / len(gndTruth)\n",
        "\n",
        "    for i in range(len(y_hat)):\n",
        "        #print(y_hat[i], \"  \", gndTruth[i])\n",
        "        if y_hat[i] != gndTruth[i]:\n",
        "            counter += 1\n",
        "    \n",
        "    error = counter / len(y_hat)\n",
        "\n",
        "    for i in range(len(gndTruth)):\n",
        "        temp +=  (y_hat[i] - gndTruthMean)**2\n",
        "    stdDev = np.sqrt(temp/len(gndTruth))\n",
        "    return error, stdDev\n",
        "\n",
        "\n",
        "#seperate samples into classes based on their labels\n",
        "def seperateData(data, labels):\n",
        "    data_dict = {0:[],              #use dictionary to store samples of different classes\n",
        "                1:[],\n",
        "                2:[],\n",
        "                3:[],\n",
        "                4:[],\n",
        "                5:[],\n",
        "                6:[],\n",
        "                7:[],\n",
        "                8:[],\n",
        "                9:[]}\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        data_dict[labels[i]].append(data[i,:])\n",
        "    return data_dict\n",
        "\n",
        "#list of means of features for samples of each class and overall mean\n",
        "def calcMeans(data_dict):\n",
        "    class_means = []                       \n",
        "    for value in data_dict.values():\n",
        "        mean = np.mean(value, axis = 0)\n",
        "        class_means.append(mean) \n",
        "    temp = sum(class_means)                              \n",
        "    overall_mean = (1/len(class_means)) * temp\n",
        "\n",
        "    return class_means, overall_mean\n",
        "\n",
        "\n",
        "#finds the between-class scatter and within-class scatter\n",
        "def calcCovariances(data_dict, class_means, overall_mean):\n",
        "    num_classes = len(class_means)\n",
        "    Si = []\n",
        "    counter1 = 0\n",
        "\n",
        "    for key, value in data_dict.items():\n",
        "        for i in range(len(value)):\n",
        "            diff = value[i] - class_means[key]\n",
        "            counter1 += np.matmul(np.transpose([diff]), [diff])\n",
        "        Si.append(counter1)\n",
        "\n",
        "    SW = sum(Si)                        #within class scatter\n",
        "\n",
        "    SB = 0                              #between class scatter\n",
        "    for i in range(num_classes):\n",
        "        diffMeans = class_means[i] - overall_mean\n",
        "        SB += len(data_dict[i]) * np.matmul(np.transpose([diffMeans]), [diffMeans])\n",
        "    \n",
        "    return SB, SW\n",
        "\n",
        "def oneHot(labels):\n",
        "    num_classes = len(set(labels))\n",
        "    onehot_labels = np.zeros((len(labels), num_classes))\n",
        "    for i in range(len(labels)):          #encodes labels to 1-hot \n",
        "        #print(i, labels[i])\n",
        "        onehot_labels[i][int(labels[i])-1] = 1\n",
        "\n",
        "    return onehot_labels\n",
        "\n",
        "# projects data along W \n",
        "def projectData(W, data):\n",
        "    #print(np.shape(W))\n",
        "    #print(np.shape(data))\n",
        "    projectedData = []\n",
        "    for i in range(len(data)):\n",
        "        #projectedData.append(data[i] @ np.transpose(W))\n",
        "        projectedData.append(data[i] @ W)\n",
        "    return projectedData\n",
        "\n",
        "#finds mean, cov, and prior for each class\n",
        "def findGaussParams(data_dict, labels, W):\n",
        "    means = []\n",
        "    covs = []\n",
        "    priors = []\n",
        "    total_cov = 0\n",
        "\n",
        "    samples = len(labels)\n",
        "    for key, value in data_dict.items():\n",
        "        projection = projectData(W, value)\n",
        "        means.append(np.mean(projection, axis = 0))\n",
        "        cov = np.cov(projection, rowvar = False)      #2x2\n",
        "        covs.append(cov)\n",
        "        prior = len(data_dict[key])/samples\n",
        "        total_cov += prior * cov\n",
        "        priors.append(prior)\n",
        "        \n",
        "    return means,covs,priors, total_cov\n",
        "\n",
        "def softmax(vect):\n",
        "    e = np.exp(vect)\n",
        "    return e / e.sum()\n",
        "\n",
        "\n",
        "def plot(projdata, labels):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    ax.scatter(projdata[:,0],projdata[:,1] , c=labels, lw=0)\n",
        "\n",
        "#finds the optimal W matrix to project data to 2D\n",
        "#finds mean, cov, and prior of each class\n",
        "def trainLDA2D(train_data, labels, test_data):\n",
        "    num_classes = len(set(labels))\n",
        "    data_dict = seperateData(train_data, labels)\n",
        "    labels_onehot = oneHot(labels)\n",
        "\n",
        "    class_means, overall_mean = calcMeans(data_dict)\n",
        "    SB,SW = calcCovariances(data_dict, class_means, overall_mean)   \n",
        "    \n",
        "    eigVal, eigVect = np.linalg.eig(np.linalg.pinv(SW) @ SB)\n",
        "    eigPairs = [(abs(eigVal[i]), eigVect[:,i]) for i in range(len(eigVal))]       #combine the eigval & eigvect into pairs\n",
        "    sortedEigPairs = sorted(eigPairs, key = lambda x: x[0], reverse = True)       #sort the eigen pairs in descending order \n",
        "    largest = sortedEigPairs[0][1]\n",
        "    seclargest = sortedEigPairs[1][1]\n",
        "    W = np.transpose(np.vstack((largest,seclargest)))                              #W formed by  eigenvects corresponding to the 2 largest eigenvals\n",
        "    #print(np.shape(W))\n",
        "\n",
        "    proj = projectData(W, train_data)\n",
        "    proj = np.asarray(proj)\n",
        "    #projplot = plot(proj, labels)\n",
        "\n",
        "    proj_class_mean, proj_class_cov, priors, total_cov = findGaussParams(data_dict, labels, W)      #find mean,cov, prior for each class\n",
        "\n",
        "    return W, proj_class_mean, proj_class_cov, priors\n",
        "\n",
        "\n",
        "def predict(data, W, means, covs, priors):\n",
        "    y_hat = []\n",
        "    proj = projectData(W, data)  \n",
        "    proj = np.asarray(proj)\n",
        "    for sample in proj:\n",
        "        sample_pred = []\n",
        "        for i in range(len(means)):                     #use the un-simplified discriminant form \n",
        "            invCov = np.linalg.inv(covs[i])\n",
        "            t1 = (-1/2) * invCov\n",
        "            t2 = invCov @ means[i]\n",
        "            t3 = (-1/2) * means[i] @ invCov @ np.transpose([means[i]]) - (1/2)*np.log(np.linalg.det(covs[i])) + np.log(priors[i])\n",
        "            g = (sample @ t1 @ np.transpose([sample])) + sample @ t2 + t3\n",
        "            sample_pred.append(g)\n",
        "        pred = softmax(sample_pred)\n",
        "        y_hat.append(np.argmax(pred))\n",
        "    \n",
        "    return y_hat\n",
        "\n",
        "def LDA2dGaussGM(filename, num_crossval):\n",
        "    split_data = crossValSplit(filename)\n",
        "    \n",
        "    globalError = 0\n",
        "    globalStdDev = 0\n",
        "\n",
        "    fold_error = []\n",
        "    fold_stddev = []\n",
        "    for i in range(len(split_data)):\n",
        "        test_data = np.copy(split_data[i])\n",
        "        test_labels = test_data[:,-1]\n",
        "        test_data = test_data[:,:-1]\n",
        "        \n",
        "        train_data_folds = np.delete(np.copy(split_data),i,0)\n",
        "        folds, samples, features = np.shape(train_data_folds)\n",
        "        train_data = np.reshape(train_data_folds, (folds * samples, features))\n",
        "        train_labels = train_data[:,-1]\n",
        "        train_data = train_data[:,:-1]\n",
        "\n",
        "        W, means, covs, priors = trainLDA2D(train_data, train_labels, test_data)\n",
        "        \n",
        "        trainPred = predict(train_data, W, means, covs, priors)\n",
        "        trainError, trainStdDev = errorAndStdDev(trainPred, train_labels)\n",
        "        \n",
        "        testPred = predict(test_data, W, means, covs, priors)\n",
        "        testError, testStdDev = errorAndStdDev(testPred, test_labels)\n",
        "\n",
        "        print(\"Fold= \", i, \"  Train error= \", trainError * 100, \"%  Train std dev= \", trainStdDev, \"  Test Error\", testError * 100, \"%   Test std dev= \", testStdDev)\n",
        "        globalError += testError \n",
        "        globalStdDev += testStdDev\n",
        "    \n",
        "    print(\"Avg Error = \", globalError / num_crossval * 100, \"%\")\n",
        "    print(\"Avg Standard Deviation = \", globalStdDev / num_crossval)\n",
        "\n",
        "dataset = chooseDataset(\"digits\")\n",
        "LDA2dGaussGM(dataset, 10)"
      ],
      "metadata": {
        "id": "YlcH_MQmJUK3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ef88ed0-4657-4639-f4b2-5ad2af911070"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold=  0   Train error=  0.0 %  Train std dev=  2.847422211959149   Test Error 0.0 %   Test std dev=  2.8913623571580835\n",
            "Fold=  1   Train error=  0.0 %  Train std dev=  2.8465806774124474   Test Error 0.0 %   Test std dev=  2.8974223603182105\n",
            "Fold=  2   Train error=  0.0 %  Train std dev=  2.844845258961805   Test Error 0.0 %   Test std dev=  2.8510454942177907\n",
            "Fold=  3   Train error=  0.0 %  Train std dev=  2.853032266255877   Test Error 0.0 %   Test std dev=  2.835007011372856\n",
            "Fold=  4   Train error=  1.0552451893234016 %  Train std dev=  2.8598619286982117   Test Error 2.793296089385475 %   Test std dev=  2.848274590684785\n",
            "Fold=  5   Train error=  0.0 %  Train std dev=  2.8596607699452523   Test Error 0.0 %   Test std dev=  2.7210087775522926\n",
            "Fold=  6   Train error=  0.0 %  Train std dev=  2.8435021794795357   Test Error 0.0 %   Test std dev=  2.9172925460317405\n",
            "Fold=  7   Train error=  0.0 %  Train std dev=  2.850370898275426   Test Error 0.0 %   Test std dev=  2.8642714709991974\n",
            "Fold=  8   Train error=  0.0 %  Train std dev=  2.861545017740103   Test Error 0.0 %   Test std dev=  2.7439780313391755\n",
            "Fold=  9   Train error=  0.0 %  Train std dev=  2.8562684466805424   Test Error 0.0 %   Test std dev=  2.7955409973508316\n",
            "Avg Error =  0.27932960893854747 %\n",
            "Avg Standard Deviation =  2.8365203637024967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logist Regression (Working)"
      ],
      "metadata": {
        "id": "SBsM6-TcQc4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from random import randrange\n",
        "from collections import defaultdict\n",
        "\n",
        "#split data into 10-folds\n",
        "def crossValSplit(dataset, folds = 10):\n",
        "    fold_size = int(len(dataset) / folds)\n",
        "    split_dataset = []\n",
        "    dataset_copy = dataset\n",
        "    for i in range(folds):\n",
        "        each_fold = []\n",
        "        while len(each_fold) < fold_size:\n",
        "            randSample = randrange(len(dataset_copy[0]))\n",
        "            each_fold.append(dataset_copy[randSample])\n",
        "            np.delete(dataset_copy,randSample)\n",
        "        split_dataset.append(each_fold)\n",
        "    \n",
        "    return split_dataset \n",
        "\n",
        "def errorAndStdDev(y_hat, gndTruth):\n",
        "    counter = 0\n",
        "    temp = 0\n",
        "    gndTruthMean = sum(gndTruth) / len(gndTruth)\n",
        "    for i in range(len(y_hat)):\n",
        "        #print(\"y_hat[i]\", y_hat[i], \"  gndTruth[i]\", gndTruth[i])\n",
        "        if y_hat[i] != gndTruth[i]:\n",
        "            counter += 1\n",
        "    \n",
        "    error = counter / len(y_hat)\n",
        "    \n",
        "    for i in range(len(gndTruth)):\n",
        "        temp +=  (y_hat[i] - gndTruthMean)**2\n",
        "    stdDev = np.sqrt(temp/len(gndTruth))\n",
        "    return error, stdDev\n",
        "\n",
        "#seperate samples into classes based on their labels\n",
        "def seperateData(data, labels):\n",
        "    data_combined = [(labels[i], data[i]) for i in range(len(labels))]\n",
        "    data_dict = defaultdict(list)\n",
        "\n",
        "    for key, val in data_combined:        #seperates data based on class\n",
        "        data_dict[key].append(val)\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "def sigmoid(z):\n",
        "    ans = 1/(1 + np.exp(-z)) \n",
        "    return ans\n",
        "\n",
        "def findCost(train_labels, z):\n",
        "    c = 1e-10\n",
        "    samples = len(train_labels)\n",
        "\n",
        "    cost_y0 = -1 * np.transpose([train_labels]) * np.log(z +c)\n",
        "    cost_y1 = (1 - np.transpose([train_labels])) * np.log(1 - z +c)\n",
        "    \n",
        "    loss = (1/samples) * np.sum(cost_y0 + cost_y1)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def classifyBinary(data, W):\n",
        "    pred = sigmoid(data @ W)\n",
        "    return pred\n",
        "\n",
        "def classifyMulti(data, W):\n",
        "    data = np.insert(data, 0, 1, axis=1)\n",
        "    W = np.reshape(W, (-1, np.shape(data)[1]))\n",
        "    pred = np.argmax(sigmoid(data @ np.transpose(W)), axis = 1)\n",
        "\n",
        "    return pred\n",
        "\n",
        "def trainBinaryLogReg(train_data, train_labels):\n",
        "    \n",
        "    num_features = np.shape(train_data)[1]\n",
        "    num_samples = len(train_data)\n",
        "    W = np.random.normal(0, 1, size=(num_features,1))\n",
        "    lr = 0.1                                          #learning rate and iteration values were found through trial and error\n",
        "    iters = 1750\n",
        "    error_history = []\n",
        "    for i in range(iters):\n",
        "        \n",
        "        z = sigmoid(train_data @ W)\n",
        "        \n",
        "        deltaE = (1/num_samples) * (np.transpose(train_data) @ (np.transpose([train_labels]) - z))\n",
        "        W += lr * deltaE\n",
        "\n",
        "        error = findCost(train_labels, z)\n",
        "        \n",
        "        error_history.append(error)\n",
        "    \n",
        "    return W, error_history\n",
        "\n",
        "#use 1-vs-all for multi-class logistic regression\n",
        "def trainMulticlassLogReg(train_data, train_labels, num_classes):\n",
        "    \n",
        "    num_samples = len(train_data)\n",
        "    lr = 0.1\n",
        "    iters = 500\n",
        "    error_history = []\n",
        "    train_data=np.insert(train_data, 0 ,1, axis = 1)\n",
        "    weights = []\n",
        "    \n",
        "    for c in range(num_classes):\n",
        "        oneVall_y = np.where(train_labels == c, 1, 0)\n",
        "        w = np.random.normal(0, 1, size=(np.shape(train_data)[1],1))\n",
        "\n",
        "        for i in range(iters):\n",
        "\n",
        "            z  = sigmoid(train_data @ w) \n",
        "  \n",
        "            grad = (1/num_samples) * (np.transpose(train_data) @ (np.transpose([oneVall_y]) - z))\n",
        "\n",
        "            w += lr * grad\n",
        "\n",
        "            error = findCost(oneVall_y, z)\n",
        "            error_history.append(error)\n",
        "        \n",
        "        weights.append(w)\n",
        "  \n",
        "    weights = np.asarray(weights)\n",
        "    return weights, error_history\n",
        "\n",
        "\n",
        "def splitDataTrainTest(data):\n",
        "    data = np.asarray(data)\n",
        "    perm_data = np.random.permutation(data)\n",
        "    train_size = int(0.8 * len(data))\n",
        "    train_split = np.asarray(perm_data[:train_size])\n",
        "    test_split = np.asarray(perm_data[train_size:])\n",
        "\n",
        "    return train_split, test_split\n",
        "\n",
        "def trainPercent(train_split, percent):\n",
        "    train_split = np.asarray(train_split)\n",
        "    perm_train_split = np.random.permutation(train_split)\n",
        "    if percent != 100:\n",
        "        train_size = int(percent/100 * len(perm_train_split))\n",
        "    else:\n",
        "        return train_split\n",
        "    \n",
        "    train_data = perm_train_split[:train_size,:]\n",
        "    return train_data\n",
        "\n",
        "\n",
        "def logisticRegression(filename, dataset, num_splits, percent):\n",
        "    split_data = crossValSplit(dataset)\n",
        "    \n",
        "    globalError = 0\n",
        "    globalStdDev = 0\n",
        "\n",
        "    fold_error = []\n",
        "    fold_stddev = []\n",
        "    gen_error_history = []\n",
        "    for i in range(len(split_data)):\n",
        "        gen_error_split = []\n",
        "        num_samples = len(split_data[i])\n",
        "\n",
        "        train_split, test_split = splitDataTrainTest(split_data[i])\n",
        "        #print(\"train split len\", len(train_split))\n",
        "        test_labels = test_split[:,-1]\n",
        "        \n",
        "        num_classes = len(set(test_labels))               #sometimes there's a skewed split where not all of the classes will be included\n",
        "        if num_classes > 2:                               #I'm hard coding the classes to avoid running into errors caused by skewed class splits\n",
        "            num_classes = 10\n",
        "        else:\n",
        "            num_classes = 2\n",
        "\n",
        "        test_data = test_split[:,:-1]\n",
        "\n",
        "        for j in percent:\n",
        "\n",
        "            train_percent = trainPercent(train_split, j)\n",
        "            train_labels = train_percent[:,-1]\n",
        "            \n",
        "            train_data = train_percent[:,:-1]\n",
        "\n",
        "            if num_classes <=2:\n",
        "                W, cost_history = trainBinaryLogReg(train_data, train_labels)\n",
        "                trainPred = classifyBinary(train_data, W)\n",
        "                testPred = classifyBinary(test_data,W)\n",
        "\n",
        "            else:\n",
        "                W, cost_history = trainMulticlassLogReg(train_data, train_labels, num_classes)\n",
        "                trainPred = classifyMulti(train_data, W)\n",
        "                testPred = classifyMulti(test_data,W)\n",
        "\n",
        "            trainError, trainStdDev = errorAndStdDev(trainPred, train_labels)\n",
        "            #print(trainError)\n",
        "            \n",
        "            testError, testStdDev = errorAndStdDev(testPred, test_labels)\n",
        "            #print(\"percent=\",j,\"train error=\", trainError * 100,\"  test error=\", testError * 100)\n",
        "\n",
        "            gen_error_split.append(testError)\n",
        "        #print(\"   \")\n",
        "        gen_error_history.append(gen_error_split)\n",
        "\n",
        "    avg_error = np.mean(gen_error_history, axis = 0)\n",
        "    error_stddev = np.std(gen_error_history, axis = 0)\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.errorbar(percent, avg_error ,error_stddev,fmt='-o')\n",
        "    plt.title(filename + \" Error Plot\")\n",
        "\n",
        "\n",
        "percent = [10,25,50,75,100]\n",
        "datasetNames = [\"Boston50\", \"Boston75\", \"Digits\"]\n",
        "#datasetNames = [\"Digits\"]\n",
        "#datasetNames = [\"Boston50\", \"Boston75\"]   #[\"Boston50\"]\n",
        "for i in datasetNames:\n",
        "    dataset = chooseDataset(i)\n",
        "    logisticRegression(i, dataset, 10, percent)"
      ],
      "metadata": {
        "id": "c2yz5VbrQgNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# naiveBayesGaussian (Working)"
      ],
      "metadata": {
        "id": "7SUq0SsqdFjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from random import randrange\n",
        "from collections import defaultdict\n",
        "\n",
        "#split data into 10-folds\n",
        "def crossValSplit(dataset, folds = 10):\n",
        "    fold_size = int(len(dataset) / folds)\n",
        "    split_dataset = []\n",
        "    dataset_copy = dataset\n",
        "    for i in range(folds):\n",
        "        each_fold = []\n",
        "        while len(each_fold) < fold_size:\n",
        "            randSample = randrange(len(dataset_copy[0]))\n",
        "            each_fold.append(dataset_copy[randSample])\n",
        "            np.delete(dataset_copy,randSample)\n",
        "        split_dataset.append(each_fold)\n",
        "    \n",
        "    return split_dataset \n",
        "\n",
        "def errorAndStdDev(y_hat, gndTruth):\n",
        "    counter = 0\n",
        "    temp = 0\n",
        "    gndTruthMean = sum(gndTruth) / len(gndTruth)\n",
        "    for i in range(len(y_hat)):\n",
        "        #print(\"y_hat[i]\", y_hat[i], \"  gndTruth[i]\", gndTruth[i])\n",
        "        if y_hat[i] != gndTruth[i]:\n",
        "            counter += 1\n",
        "    \n",
        "    error = counter / len(y_hat)\n",
        "    \n",
        "    for i in range(len(gndTruth)):\n",
        "        temp +=  (y_hat[i] - gndTruthMean)**2\n",
        "    stdDev = np.sqrt(temp/len(gndTruth))\n",
        "    return error, stdDev\n",
        "\n",
        "#seperate samples into classes based on their labels\n",
        "def seperateData(data, labels):\n",
        "    data_combined = [(labels[i], data[i]) for i in range(len(labels))]\n",
        "    data_dict = defaultdict(list)\n",
        "\n",
        "    for key, val in data_combined:        #seperates data based on class\n",
        "        data_dict[key].append(val)\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "#find mean, std dev, prior for training data which will be used to later to find the likelihood\n",
        "def calcParams(data_dict, num_classes, num_features):\n",
        "    class_means = np.zeros((num_classes,num_features))\n",
        "    class_stds = np.zeros((num_classes,num_features))\n",
        "    priors = np.zeros((num_classes,1))\n",
        "\n",
        "    for key,val in data_dict.items():\n",
        "        class_means[int(key),:] = np.mean(val, axis = 0)\n",
        "        class_stds[int(key),:] = np.std(val, axis = 0)\n",
        "        priors[int(key)] = len(val)\n",
        "\n",
        "    smallvalmean = 1e-10 * np.ones((np.shape(class_means)))       #add some small values to avoid dividing by 0 and other complications\n",
        "    smallvalstd = 1e-10 * np.ones((np.shape(class_stds)))\n",
        "    class_means += smallvalmean\n",
        "    class_stds += smallvalstd\n",
        "\n",
        "    return class_means, class_stds, priors\n",
        "\n",
        "#finds likelihood of sample using mean & std dev found in \"training\"\n",
        "def calcLikelihood(sample,mean, std, num_classes):\n",
        "    likelihood = np.sum(np.divide((sample - mean), std) ** 2)               #element-wise division for each feature\n",
        "    \n",
        "    return likelihood \n",
        "\n",
        "#classify sample using naive bayes by multiplying the likelihood and ln(prior) \n",
        "def classify(sample,class_means, class_stds, priors):\n",
        "    pred = []\n",
        "    num_classes = len(class_means)\n",
        "    for c in range(num_classes):                #find the probability for each class \n",
        "        likelihood = calcLikelihood(sample,class_means[c], class_stds[c], num_classes)\n",
        "        g = (-1/2) * likelihood + np.log(priors[c])\n",
        "        pred.append(g)\n",
        "    y_hat = np.argmax(pred)                     #predicted value is the index of the max probability \n",
        "    return y_hat \n",
        "\n",
        "\n",
        "#generate class predictions\n",
        "def predict(data, class_means, class_stds, priors):\n",
        "    y_hat = []\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        prediction = classify(data[i], class_means, class_stds, priors)\n",
        "        y_hat.append(prediction)\n",
        "    return y_hat \n",
        "\n",
        "#finds parameters needed for naive bayes \n",
        "def trainNaiveBayes(train_data, train_labels, num_classes):\n",
        "    num_features = len(train_data[0])\n",
        "    data_dict = seperateData(train_data, train_labels)\n",
        "    class_means, class_stds, priors = calcParams(data_dict, num_classes, num_features)\n",
        "    priors = priors / len(train_data)\n",
        "    return class_means, class_stds, priors\n",
        "\n",
        "#split total data into train(80%) and test(20%) sets\n",
        "def splitDataTrainTest(data):             \n",
        "    data = np.asarray(data)\n",
        "    perm_data = np.random.permutation(data)\n",
        "    train_size = int(0.8 * len(data))\n",
        "    train_split = np.asarray(perm_data[:train_size])\n",
        "    test_split = np.asarray(perm_data[train_size:])\n",
        "\n",
        "    return train_split, test_split\n",
        "\n",
        "#partition training set to be [10%, 25%, 50%, 75%, 100%] of the total training set size\n",
        "def trainPercent(train_split, percent):         \n",
        "    train_split = np.asarray(train_split)\n",
        "    perm_train_split = np.random.permutation(train_split)\n",
        "    if percent != 100:\n",
        "        train_size = int(percent/100 * len(perm_train_split))\n",
        "    else:\n",
        "        return train_split \n",
        "    train_data = perm_train_split[:train_size,:]\n",
        "    return train_data\n",
        "\n",
        "\n",
        "def naiveBayesGaussian(filename, dataset, num_splits, percent):\n",
        "    split_data = crossValSplit(dataset)\n",
        "    \n",
        "    globalError = 0\n",
        "    globalStdDev = 0\n",
        "    \n",
        "    fold_error = []\n",
        "    fold_stddev = []\n",
        "    gen_error_history = []\n",
        "    for i in range(len(split_data)):\n",
        "        gen_error_split = []\n",
        "        num_samples = len(split_data[i])\n",
        "\n",
        "        train_split, test_split = splitDataTrainTest(split_data[i])\n",
        "        test_labels = test_split[:,-1]\n",
        "        num_classes = len(set(test_labels))\n",
        "        if num_classes > 2:\n",
        "            num_classes = 10\n",
        "        else:\n",
        "            num_classes = 2\n",
        "\n",
        "        test_data = test_split[:,:-1]\n",
        "\n",
        "        for j in percent:\n",
        "            train_percent = trainPercent(train_split, j)\n",
        "            train_labels = train_percent[:,-1]\n",
        "            train_data = train_percent[:,:-1]\n",
        "\n",
        "            class_means, class_stds, priors = trainNaiveBayes(train_data, train_labels, num_classes)\n",
        "\n",
        "            trainPred = predict(train_data, class_means, class_stds, priors)\n",
        "            trainError, trainStdDev = errorAndStdDev(trainPred, train_labels)\n",
        "            \n",
        "            testPred = predict(test_data,class_means, class_stds, priors)\n",
        "            testError, testStdDev = errorAndStdDev(testPred, test_labels)\n",
        "            print(\"percent=\",j,\"train error=\", trainError * 100,\"  test error=\", testError * 100)\n",
        "\n",
        "            gen_error_split.append(testError)\n",
        "        \n",
        "        gen_error_history.append(gen_error_split)\n",
        "\n",
        "    avg_error = np.mean(gen_error_history, axis = 0)\n",
        "    error_stddev = np.std(gen_error_history, axis = 0)\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.errorbar(percent, avg_error ,error_stddev,fmt='-o')\n",
        "    plt.title(filename + \" Error Plot\")\n",
        "\n",
        "percent = [10,25,50,75,100]\n",
        "#datasetNames = [\"digits\"]\n",
        "datasetNames = [\"Boston50\", \"Boston75\", \"digits\"]   #[\"Boston50\"]\n",
        "for i in datasetNames:\n",
        "    dataset = chooseDataset(i)\n",
        "    naiveBayesGaussian(i, dataset, 10, percent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tU_TGgmjdNvK",
        "outputId": "7846c796-d30c-4d0b-c6cb-abd6575ab59b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "percent= 10 train error= 0.0   test error= 70.0\n",
            "percent= 25 train error= 0.0   test error= 60.0\n",
            "percent= 50 train error= 5.0   test error= 20.0\n",
            "percent= 75 train error= 16.666666666666664   test error= 0.0\n",
            "percent= 100 train error= 5.0   test error= 0.0\n",
            "percent= 10 train error= 0.0   test error= 30.0\n",
            "percent= 25 train error= 0.0   test error= 0.0\n",
            "percent= 50 train error= 20.0   test error= 20.0\n",
            "percent= 75 train error= 30.0   test error= 20.0\n",
            "percent= 100 train error= 10.0   test error= 20.0\n",
            "percent= 10 train error= 0.0   test error= 40.0\n",
            "percent= 25 train error= 10.0   test error= 20.0\n",
            "percent= 50 train error= 10.0   test error= 10.0\n",
            "percent= 75 train error= 10.0   test error= 0.0\n",
            "percent= 100 train error= 12.5   test error= 10.0\n",
            "percent= 10 train error= 0.0   test error= 20.0\n",
            "percent= 25 train error= 0.0   test error= 10.0\n",
            "percent= 50 train error= 5.0   test error= 0.0\n",
            "percent= 75 train error= 6.666666666666667   test error= 20.0\n",
            "percent= 100 train error= 12.5   test error= 20.0\n",
            "percent= 10 train error= 25.0   test error= 30.0\n",
            "percent= 25 train error= 10.0   test error= 20.0\n",
            "percent= 50 train error= 15.0   test error= 10.0\n",
            "percent= 75 train error= 23.333333333333332   test error= 10.0\n",
            "percent= 100 train error= 22.5   test error= 10.0\n",
            "percent= 10 train error= 0.0   test error= 30.0\n",
            "percent= 25 train error= 0.0   test error= 0.0\n",
            "percent= 50 train error= 5.0   test error= 0.0\n",
            "percent= 75 train error= 3.3333333333333335   test error= 0.0\n",
            "percent= 100 train error= 2.5   test error= 0.0\n",
            "percent= 10 train error= 0.0   test error= 40.0\n",
            "percent= 25 train error= 0.0   test error= 10.0\n",
            "percent= 50 train error= 10.0   test error= 10.0\n",
            "percent= 75 train error= 16.666666666666664   test error= 10.0\n",
            "percent= 100 train error= 12.5   test error= 10.0\n",
            "percent= 10 train error= 0.0   test error= 40.0\n",
            "percent= 25 train error= 0.0   test error= 30.0\n",
            "percent= 50 train error= 5.0   test error= 10.0\n",
            "percent= 75 train error= 13.333333333333334   test error= 20.0\n",
            "percent= 100 train error= 15.0   test error= 20.0\n",
            "percent= 10 train error= 0.0   test error= 30.0\n",
            "percent= 25 train error= 0.0   test error= 20.0\n",
            "percent= 50 train error= 5.0   test error= 10.0\n",
            "percent= 75 train error= 6.666666666666667   test error= 20.0\n",
            "percent= 100 train error= 7.5   test error= 20.0\n",
            "percent= 10 train error= 0.0   test error= 40.0\n",
            "percent= 25 train error= 10.0   test error= 20.0\n",
            "percent= 50 train error= 10.0   test error= 20.0\n",
            "percent= 75 train error= 23.333333333333332   test error= 20.0\n",
            "percent= 100 train error= 22.5   test error= 20.0\n",
            "percent= 10 train error= 0.0   test error= 30.0\n",
            "percent= 25 train error= 0.0   test error= 0.0\n",
            "percent= 50 train error= 0.0   test error= 20.0\n",
            "percent= 75 train error= 3.3333333333333335   test error= 20.0\n",
            "percent= 100 train error= 7.5   test error= 20.0\n",
            "percent= 10 train error= 0.0   test error= 40.0\n",
            "percent= 25 train error= 0.0   test error= 30.0\n",
            "percent= 50 train error= 10.0   test error= 20.0\n",
            "percent= 75 train error= 0.0   test error= 20.0\n",
            "percent= 100 train error= 5.0   test error= 20.0\n",
            "percent= 10 train error= 0.0   test error= 0.0\n",
            "percent= 25 train error= 0.0   test error= 10.0\n",
            "percent= 50 train error= 0.0   test error= 30.0\n",
            "percent= 75 train error= 0.0   test error= 0.0\n",
            "percent= 100 train error= 0.0   test error= 0.0\n",
            "percent= 10 train error= 0.0   test error= 10.0\n",
            "percent= 25 train error= 0.0   test error= 0.0\n",
            "percent= 50 train error= 5.0   test error= 0.0\n",
            "percent= 75 train error= 6.666666666666667   test error= 0.0\n",
            "percent= 100 train error= 5.0   test error= 0.0\n",
            "percent= 10 train error= 0.0   test error= 10.0\n",
            "percent= 25 train error= 0.0   test error= 30.0\n",
            "percent= 50 train error= 15.0   test error= 10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:78: RuntimeWarning: divide by zero encountered in log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "percent= 75 train error= 6.666666666666667   test error= 10.0\n",
            "percent= 100 train error= 12.5   test error= 10.0\n",
            "percent= 10 train error= 0.0   test error= 20.0\n",
            "percent= 25 train error= 0.0   test error= 20.0\n",
            "percent= 50 train error= 0.0   test error= 20.0\n",
            "percent= 75 train error= 16.666666666666664   test error= 30.0\n",
            "percent= 100 train error= 22.5   test error= 30.0\n",
            "percent= 10 train error= 0.0   test error= 20.0\n",
            "percent= 25 train error= 10.0   test error= 10.0\n",
            "percent= 50 train error= 10.0   test error= 10.0\n",
            "percent= 75 train error= 3.3333333333333335   test error= 10.0\n",
            "percent= 100 train error= 10.0   test error= 10.0\n",
            "percent= 10 train error= 0.0   test error= 40.0\n",
            "percent= 25 train error= 20.0   test error= 10.0\n",
            "percent= 50 train error= 15.0   test error= 10.0\n",
            "percent= 75 train error= 23.333333333333332   test error= 10.0\n",
            "percent= 100 train error= 17.5   test error= 10.0\n",
            "percent= 10 train error= 0.0   test error= 10.0\n",
            "percent= 25 train error= 10.0   test error= 0.0\n",
            "percent= 50 train error= 10.0   test error= 0.0\n",
            "percent= 75 train error= 13.333333333333334   test error= 0.0\n",
            "percent= 100 train error= 17.5   test error= 0.0\n",
            "percent= 10 train error= 0.0   test error= 0.0\n",
            "percent= 25 train error= 0.0   test error= 0.0\n",
            "percent= 50 train error= 10.0   test error= 0.0\n",
            "percent= 75 train error= 13.333333333333334   test error= 10.0\n",
            "percent= 100 train error= 15.0   test error= 10.0\n",
            "percent= 10 train error= 0.0   test error= 27.77777777777778\n",
            "percent= 25 train error= 0.0   test error= 22.22222222222222\n",
            "percent= 50 train error= 0.0   test error= 0.0\n",
            "percent= 75 train error= 0.0   test error= 0.0\n",
            "percent= 100 train error= 0.0   test error= 0.0\n",
            "percent= 10 train error= 0.0   test error= 66.66666666666666\n",
            "percent= 25 train error= 0.0   test error= 25.0\n",
            "percent= 50 train error= 0.0   test error= 2.7777777777777777\n",
            "percent= 75 train error= 0.0   test error= 2.7777777777777777\n",
            "percent= 100 train error= 0.0   test error= 2.7777777777777777\n",
            "percent= 10 train error= 0.0   test error= 63.888888888888886\n",
            "percent= 25 train error= 0.0   test error= 30.555555555555557\n",
            "percent= 50 train error= 0.0   test error= 8.333333333333332\n",
            "percent= 75 train error= 0.0   test error= 11.11111111111111\n",
            "percent= 100 train error= 0.0   test error= 5.555555555555555\n",
            "percent= 10 train error= 0.0   test error= 77.77777777777779\n",
            "percent= 25 train error= 0.0   test error= 52.77777777777778\n",
            "percent= 50 train error= 0.0   test error= 2.7777777777777777\n",
            "percent= 75 train error= 0.0   test error= 13.88888888888889\n",
            "percent= 100 train error= 0.0   test error= 5.555555555555555\n",
            "percent= 10 train error= 0.0   test error= 63.888888888888886\n",
            "percent= 25 train error= 0.0   test error= 19.444444444444446\n",
            "percent= 50 train error= 0.0   test error= 19.444444444444446\n",
            "percent= 75 train error= 0.0   test error= 5.555555555555555\n",
            "percent= 100 train error= 0.0   test error= 5.555555555555555\n",
            "percent= 10 train error= 0.0   test error= 72.22222222222221\n",
            "percent= 25 train error= 0.0   test error= 27.77777777777778\n",
            "percent= 50 train error= 0.0   test error= 22.22222222222222\n",
            "percent= 75 train error= 0.0   test error= 11.11111111111111\n",
            "percent= 100 train error= 0.0   test error= 8.333333333333332\n",
            "percent= 10 train error= 0.0   test error= 58.333333333333336\n",
            "percent= 25 train error= 0.0   test error= 30.555555555555557\n",
            "percent= 50 train error= 0.0   test error= 22.22222222222222\n",
            "percent= 75 train error= 0.0   test error= 0.0\n",
            "percent= 100 train error= 0.0   test error= 0.0\n",
            "percent= 10 train error= 0.0   test error= 55.55555555555556\n",
            "percent= 25 train error= 0.0   test error= 41.66666666666667\n",
            "percent= 50 train error= 0.0   test error= 11.11111111111111\n",
            "percent= 75 train error= 0.0   test error= 0.0\n",
            "percent= 100 train error= 0.0   test error= 0.0\n",
            "percent= 10 train error= 0.0   test error= 41.66666666666667\n",
            "percent= 25 train error= 0.0   test error= 38.88888888888889\n",
            "percent= 50 train error= 0.0   test error= 16.666666666666664\n",
            "percent= 75 train error= 0.0   test error= 0.0\n",
            "percent= 100 train error= 0.0   test error= 0.0\n",
            "percent= 10 train error= 0.0   test error= 63.888888888888886\n",
            "percent= 25 train error= 0.0   test error= 38.88888888888889\n",
            "percent= 50 train error= 0.0   test error= 11.11111111111111\n",
            "percent= 75 train error= 0.0   test error= 13.88888888888889\n",
            "percent= 100 train error= 0.0   test error= 5.555555555555555\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhU9dn/8fdNVkiAsAQkG7sICohicKt7XVrEXeCxv2pbqz6PVtu64la1Vq3W7rbVy65PFVCkgEprfdxaK7LIvsqmCUEwiqBAJCS5f3/MiUxCIEOYZJIzn9d1zdWZc745587p8ZMv9zkzY+6OiIi0fe0SXYCIiMSHAl1EJCQU6CIiIaFAFxEJCQW6iEhIKNBFREJCgS7ShpnZKWa2IdF1SOugQJcDYmbvmVmFmW03s0/M7EUzK4zDdt3MBsSpxlPMrCaosfZxedT6rmb2NzPbYWbvm9l/7Wdb95jZ7nrb2hqPOmNlZleYWXWw70/NbKGZjW7Cdv5kZvc3R43SOijQpSnOdfdsoBewGfhVgutpyEZ3z456/Dlq3WNAJdATuAz4rZkdvp9tTa63rZyGBplZaizL9mc/42cFxzwH+D3wjJl1OZBtS/gp0KXJ3P1zYAowpHaZmXU2s7+YWXkw+73TzNoF6waY2Rtmts3MPjKzycHyfwU/viiYhY4Nln/bzNaY2RYzm2FmeVH7cTO7xsxWm9lWM3vMzKyxms0sC7gIuMvdt7v7m8AM4P815RgEdVxrZquB1bUtEDO71cw2AX80swwz+7mZbQwePzezjODn9xq/v/25ew3wB6A90L+Begab2evBMVlmZmOC5VcR+eN1S3CMn2/K7yutmwJdmszMOgBjgbejFv8K6Az0A04Gvg58I1j3Q+CfQBegIBiLu58UrB8ezIAnm9lpwIPApUT+JfA+MKleCaOBY4Bhwbizotb1MLPNZrbezH4WBDnAoUCVu78bNXYRsL8ZemPOB0ax5w/bIUBXoDdwFXAHcCxwJDAcKAbujPr5+uP3KZjBXwlsB1bXW5cGPE/kGPcAvgM8ZWaD3P0J4Cng4eAYn9vUX1ZaLwW6NMW0oI+8Dfgy8AiAmaUA44AJ7v6Zu78HPMqe2e9uIqGV5+6fB7PjfbkM+IO7z3f3XcAE4Dgz6xM15iF33+ruJcBrRAITYGXwvBdwGnA08NNgXTbwab19bQM67qeWS4MZb+3jtXrrH3T3Le5eEbyuAX7g7ruCZZcB97n7h+5eDtxL3X8R1B/fkGODY74JGA9c4O7b6o8Jfr+H3L3S3V8FXgjGSxJQoEtTnB/0kTOB64A3zOwQoDuQRmQ2Xet9ID94fgtgwJygHfDN/ewjL3o77r4d+DhqWxAJt1o7iYQZ7r7J3Ze7e427rw/2e1EwbjvQqd6+OgGf7aeWZ9w9J+pxar31pfVelwftqAZ/l+B53n7GN+TtYN/d3f1Yd/+/BsbkAaVBWyZ6X/kNjJUQUqBLk7l7tbtPBaqBE4GP2DMLr1UElAXjN7n7t909D7ga+M1+7mzZGL2doGXSrXZbB1oqe871d4FUMxsYtX44sKwJ243e/v5e1/ldiByTjfsZ31QbgcLaaxZR+6o9Zvpo1ZBToEuTWcR5RHriK9y9GngG+JGZdTSz3sD3gb8G4y8xs4Lgxz8hEjC1s8nNRPrutSYC3zCzI4MLiA8As4M2TmN1nWpmvYP6CoGHgOkA7r4DmArcZ2ZZZnYCcB7wv00/Eo2aCNxpZrlm1h24m+CYxNlsIv9SucXM0szsFOBc9lx7qH+MJWQU6NIUz5vZdiK96B8Bl7t77Qz3O8AOYB3wJvA0kbsyIHIBc3bwszOAG9x9XbDuHuDPQY/60qClcBfwHPABkTs6xsVY3wjgraCOt4AlwPVR6/+HyF0iHxIJ2/+Oqr8hY+vdh77dzHrEWAvA/cA8YHFQy/xgWVy5eyWRAD+HyL+WfgN83d1XBkN+DwwJjvG0eO9fEs/0BRciIuGgGbqISEgo0EVEQkKBLiISEgp0EZGQOKAPDoqn7t27e58+fRK1exGRNumdd975yN1zG1qXsEDv06cP8+bNS9TuRUTaJDN7f1/r1HIREQkJBbqISEgo0EVEQkKBLiISEjEFupmdbWargm+Pua2B9VcE31CzMHhcGf9SRURkfxq9yyX40oLHiHyRwQZgrpnNcPfl9YZOdvfrmqFGERGJQSwz9GJgjbuvCz7NbRKRjxsVEZFWJJZAz6fuN7JsoOFvQLnIzBab2ZTgM6j3YmZXmdk8M5tXXl7ehHJFRGRf4nVR9Hmgj7sPA14G/tzQIHd/wt1HuvvI3NwG3+jUqLGPz2Ls47OaXqmISEjFEuhlQPSMu4B6XwPm7h8HX+QL8CSRL+UVEZEWFEugzwUGmllfM0sn8q0xM6IHmFmvqJdjgBXxK1FERGLR6F0u7l5lZtcBLwEpwB/cfZmZ3QfMc/cZwPVmNgaoArYAVzRjzSIi0oCYPpzL3WcCM+stuzvq+QRgQnxLExGRA6F3ioqIhIQCXUQkJBToIiIhoUAXEQkJBbqISEgo0EVEQkKBLiISEgp0EZGQUKCLiISEAl1EJCQU6CIiIaFAFxEJCQW6iEhIKNBFREJCgS4iEhIKdBGRkFCgi4iEhAJdRCQkFOgiIiGhQBcRCQkFuohISCjQRURCQoEuIhISCnQRkZBQoIuIhIQCXUQkJBToIiIhoUAXEQkJBbqISEgo0EVEQkKBLiISEgp0EZGQUKCLiIRETIFuZmeb2SozW2Nmt+1n3EVm5mY2Mn4liohILBoNdDNLAR4DzgGGAOPNbEgD4zoCNwCz412kiIg0LpYZejGwxt3XuXslMAk4r4FxPwR+DHwex/pERCRGsQR6PlAa9XpDsOwLZnYUUOjuL+5vQ2Z2lZnNM7N55eXlB1ysiIjs20FfFDWzdsBPgRsbG+vuT7j7SHcfmZube7C7FhGRKLEEehlQGPW6IFhWqyNwBPC6mb0HHAvM0IVREZGWFUugzwUGmllfM0sHxgEzale6+zZ37+7ufdy9D/A2MMbd5zVLxSIi0qBGA93dq4DrgJeAFcAz7r7MzO4zszHNXaCIiMQmNZZB7j4TmFlv2d37GHvKwZclIiIHSu8UFREJCQW6iEhIKNBFREJCgS4iEhJtKtCnLShjQclWZq/fwgkPvcq0BWWN/5CISJJoM4E+bUEZE6YuobK6BoCyrRVMmLpEoS4iEmgzgf7IS6uo2F1dZ1nF7moeeWlVgioSEWld2kygb9xacUDLRUSSTZsJ9Lyc9ge0XEQk2bSZQL/5rEG0T0vZa/klIwsSUI2ISOvTZgL9/BH5PHjhUNJTIiX36pxJTvs0ZizayOf1eusiIsmozQQ6REJ9RFEOo/p2ZdaE0/n1fx3FuvIdPPpPXRgVEWlTgV7fiQO7c9moIp58cz3z3tuS6HJERBKqTQc6wISvDCY/pz03T1lMRWXbar2MfXwWYx+flegyRCQk2nygZ2ek8vDFw1j/0Q7dky4iSa3NBzrA8f27c/lxvfnjW+uZve7jRJcjIpIQoQh0gFvPOYzCLh24ecpidlZWJbocEZEWF5pA75Ceyk8uGU7pJzv58d9XJrocEZEWF5pAByju25Urju/Dn2e9z1trP0p0OSIiLSpUgQ5wy1mH0adbB26Zspjtu9R6EZHkEbpAb5+ewk8uGU7Z1goenLki0eWIiLSY0AU6wMg+XbnyxL48NbuEN1er9SIiySGUgQ5w45mD6Jebxa3PLeazz3cnuhwRkWYX2kDPTIu0Xj7YVsEDar2ISBIIbaADHFXUhatO6s/EOaW88W55ossREWlWoQ50gO+eMZCBPbK5dcpitlWo9SIi4RX6QK9tvZRv38X9LyxPdDkiIs0m9IEOMLwwh2tO7sez72zg1ZWbE12OiEizSIpAB7j+9IEM6tmRCVOXsG2nWi8iEj5JE+gZqSk8eulwPtpeyb0vLEt0OSIicZc0gQ5wRH5nrj11AFPnl/HycrVeRCRckirQAa47dQCDe3Xi9r8t4ZMdlYkuR0QkbpIu0NNT2/HoJcP5ZEcl9zyv1ouIhEdMgW5mZ5vZKjNbY2a3NbD+GjNbYmYLzexNMxsS/1LjZ0heJ75z2kCmL9zIP5Z+kOhyRETiotFAN7MU4DHgHGAIML6BwH7a3Ye6+5HAw8BP415pnP3Pqf05PK8Td05byha1XkQkBGKZoRcDa9x9nbtXApOA86IHuPunUS+zAI9fic0jLaUdj146nG0Vu7l7+tJElyMictBiCfR8oDTq9YZgWR1mdq2ZrSUyQ7++oQ2Z2VVmNs/M5pWXJ/6zVQ47pBPfPeNQXlj8AS8uVutFRNq2uF0UdffH3L0/cCtw5z7GPOHuI919ZG5ubrx2fVCuPqkfwwo6c9f0pXy0fVeiyxERabJYAr0MKIx6XRAs25dJwPkHU1RLSk2J3PWy/fMq7pq2FPdW3y0SEWlQLIE+FxhoZn3NLB0YB8yIHmBmA6NefhVYHb8Sm9/Anh35/pmH8velm3herRcRaaMaDXR3rwKuA14CVgDPuPsyM7vPzMYEw64zs2VmthD4PnB5s1XcTL79pX6MKMrh7ulL+fCzzxNdjojIAYuph+7uM939UHfv7+4/Cpbd7e4zguc3uPvh7n6ku5/q7m3uHTsp7YxHLh7Ozspq7vibWi8i0vYk3TtF92dAj2xuPnMQLy/fzPSFGxNdjojIAVGg1/PNE/tydO8u/GDGMjZ/qtaLiLQdCvR6Iq2XYeyqqub2qUvUehGRNkOB3oB+udncctZhvLLyQ56bv787NEVEWg8F+j5ccXwfivt05d7nl/HBtopElyMi0igF+j60a2c8cskwqqqd255T60VEWj8F+n707pbFhK8cxhvvlvPMvNLGf0BEJIEU6I342qjeHNuvK/e/sIKyrWq9iEjrpUBvRLvgDUfV7tz23GK1XkSk1VKgx6Cwawdu/8pg/r36IybOUetFRFonBXqMLhtVxIkDuvOjF5dTumVnossREdmLAj1GZsZDFw3FzLj1ucXU1Kj1IiKtiwL9ABR06cCdXx3MW2s/5qnZ7ye6HBGROhToB2jsMYWcdGguD8xcScnHar2ISOuhQD9AZsaPLxpKaopx85RFar2ISKuhQG+CXp3bc9foIcxev4W/zHov0eWIiAAK9Ca75OgCTh2Uy0P/WMl7H+1IdDkiIgr0pjIzHrxwGOkp7bjp2UVUq/UiIgnW5gJ98tXHMfnq4xJdBgCHdM7knjGHM+/9T/jjf9YnuhwRSXJtLtBbmwtG5HPG4J488tIq1pZvT3Q5IpLEFOgHycx44MIjaJ+ews1qvYhIAinQ46BHx0zuHXM480u28vs31yW6HBFJUgr0OBkzPI+zDu/JT/75Lms+/CzR5YhIElKgx4mZcf/5Q8lKT+HGZxdTVV2T6JJEJMko0OMot2MGPzz/CBaVbuWJf6v1IiItS4EeZ6OH5fHVob34+curWbVJrRcRaTkK9GZw33mH0zEzlZueXcRutV5EpIUo0JtBt+wM7j//CJaUbeN3r69NdDkikiQU6M3knKG9GDM8j1++upoVH3ya6HJEJAko0JvRvWMOp3P7dG58Rq0XEWl+CvRm1CUrnQcuOILlH3zKY6+tSXQ5IhJyCvRmdubhh3DBiHx+/eoalpZtS3Q5IhJiCvQW8INzh9A1K52bnl1EZZVaLyLJbOzjsxj7+Kxm2XZMgW5mZ5vZKjNbY2a3NbD++2a23MwWm9krZtY7/qW2XTkd0nnwwqGs3PQZv3p1daLLEYlZc4aPxF+jgW5mKcBjwDnAEGC8mQ2pN2wBMNLdhwFTgIfjXWhbd/rgnlx8dAG/eX0tizdsTXQ5IhJCsczQi4E17r7O3SuBScB50QPc/TV33xm8fBsoiG+Z4XDX6CHkZmdw07OL2FVVnehyRCRkYgn0fKA06vWGYNm+fAv4e0MrzOwqM5tnZvPKy8tjrzIkOrdP46GLhvLu5u384v/UehGR+IrrRVEz+xowEnikofXu/oS7j3T3kbm5ufHcdZtxyqAejDumkN+8vpb5JZ8we/0WTnjoVaYtKEt0aSLSxsUS6GVAYdTrgmBZHWZ2BnAHMMbdd8WnvHA6srAzBuyujny7UdnWCiZMXaJQF5GDEkugzwUGmllfM0sHxgEzogeY2QjgcSJh/mH8ywyXX726lvpfVFexu5pHXlqVkHpEJBwaDXR3rwKuA14CVgDPuPsyM7vPzMYEwx4BsoFnzWyhmc3Yx+YE2Li1osHlZVsr+Hy3LpaKSNOkxjLI3WcCM+stuzvq+RlxrivU8nLaU7aPUB/1wCtceFQ+44uLOLRnxxauTETaMr1TNAFuPmsQ7dNS6ixrn9aO/zmlP18a2J2/vv0+Z/7sX1z027eY8s4GKio1axeRxsU0Q5f4On9E5K7PW6YsprK6hvyc9tx81qAvln+8fRdT55cxcU4JNz27iHufX8YFIyKz9sG9OiWydBFpxRToCXL+iHwmzikBYPLVx9VZ1y07g2+f1I8rv9SXOeu3MHFOCZPmlvKXWe9zZGEO44sLGT0sj6wM/d9XX+3b1OsfU5FkoERoxcyMUf26MapfN+7ZWfnFrP3W55bwwxdWcN6ReYwvLuKI/M6JLlVEWgEFehuR0yGdb57Yl2+c0Id33v+EiXNKmfLOBp6aXcLQ/M6MLy5izJF5ZGvWLpK09F9/G2NmjOzTlZF9unL36CFMWxiZtd/+tyXc/+JyxgyPzNqHFXTGzBJdroi0IAV6G9a5QxqXH9+Hrx/Xm4WlW5k4p4TpCzcyaW4pQ3p1YnxxIeeNyKdTZlqiSxWRFqBADwEzY0RRF0YUdeGu0UOYvnAjT88u4a7py/jRzBWcOyyPccVFHFWUo1m7SIgp0EOmY2YaXzu2N5eNKmJJ2TYmzillxsIynn1nA4N6dmR8cSEXjCigcwfN2kXCRoEeUmbGsIIchhXkcMdXB/P8oo1MmlPCPc8v58G/r+SrQ3sxflQRI3t30axdJCQU6EkgOyOV8cVFjC8uYmnZNibNLWH6go1MXVDGgB7ZjDumkIuOKqBLVnqiSxWRg6C3/ieZI/I7c//5Q5l9x+k8fPEwOmamcv+LKxj1wCtcP3EBs9Z+jHv9z4IUkbZAM/Qk1SE9lUtHFnLpyEJWbvqUSXNKmTp/AzMWbaRf9yzGHlPIRUcX0D07I9GlikiMNEMXDjukE/eMOZw5d5zBTy8dTrfsdB78+0qOe/AVrn16Pv9Z8xE1NZq1i7R2mqHLFzLTUrjwqAIuPKqA1Zs/Y+KcUqYu2MCLiz+gqGsHxhUXcvHRBfTomJnoUkWkAZqhS4MG9uzI3ecO4e0Jp/OLcUfSq3MmD/9jFcc/+CrX/O87vPFuuWbtIq2MZuiyX5lpKZx3ZD7nHZnP2vLtTJ4b+QyZfyzbREGX9owdWcilxxTSs5Nm7SKJpkCXmPXPzeb2rwzmxjMP5eXlm5k4p4RHX36Xn7+ymtMO68H44kJOPrQHKe10X7tIIijQ5YBlpKYwelgeo4fl8d5HO5g8r5Rn55Xy8vLN5HXO5NJjInfP5OW0T3SpIklFgS4HpU/3LG49+zC+d8ahvLJiMxPnlvKLV1bzy1dWc8qgHowvLuLUQbmkpuhyjUhzU6BLXKSntuOcob04Z2gvSrfsZPLcUp6ZV8q3/zKPnp0yvrjnvbBrh0SXKpIw0xaUsaBkK5XVNZzw0Kt1vnoyHhToEneFXTtw01mD+O4ZA3l15YdMnFPCr19bw69fW8NJA3MZX1zI6YN7kqZZuySRaQvKmDB1CZXVNQCUba1gwtQlAHELdQW6NJvUlHacefghnHn4IZRtreCZYNZ+zV/n0z07g0tHFjDumCKKumnWLm2fu1Oxu5rtu6rYsauaHbuqIo/KKrbvquaeGcuo2F1d52cqdlfzyEurFOjStuTntOd7Xz6U75w2gDfeLWfinFJ+98ZafvP6Wk4c0J3xxUV8eUhP0lM1a5eWUVPj7NwdCd7tu6rYuas2jCMhXBvK23dVsTMI5R0NrP/ieWUVTfkYpI1bK+L2OynQpUWlprTj9ME9OX1wTz7YVsGz8zYweW4p1z49n25Z6Vx8dAFjjymkX252oktNes3d7z1Q1TUehGftIzpwo8J4VxU7KhteV/95rDqkp9AhPZXsjBSyMlLJykile3Y6vbt1IDsjda91WRkpZKWnRtZlRNZ97cnZbPp0117bjufdYAp0SZhendtz/ekDufbUAfx7dTkT55Tw5Jvrefxf6zi2X1fGFxdx9hGHkJGakuhSk048+r1V1TXs2FXN9soqdgbhWjuTrQ3e7buqg9lv3SCOXlcbzJ/vrolpv2aQlZ5Kh/QUsoOA7ZCewiGdMvcEbnokfCOBG4xLj3peuy4I8ni8t+K2cwYzYeqSOm2X9mkp3HzWoIPedi0FuiRcSjvjlEE9OGVQDz789HOefWcDk+aWcMOkhXTpkMaFRxUwvriQAT06JrrUUKusqvmihfDAzBUN9nvvnr6U1R9+Fgnq/bQitu+qorIqtgBuFwRw7cy2dsabn5NOdkZKMMNNDcbsmQVnZ9TOmuuGdPu0FNq1wje31f4hvGXKYiqra8jPaa+7XCTcenTK5NpTB/DfJ/fnrbUfM3FOCX9+6z1+/+Z6ivt0ZfyoQs45oheZack9a3d3dlXVsDOqtbBXqyF4vacVEd3z3RO8Oyoj/ePa2fj+fPp5Fb97Yx1Z0bPfIFy7ZXXY027ISCU7fc+6rC8CuW5oZ2ekkpnWLmm+Nev8EflMnFMCwOSrj4v79hXo0iq1a2ecOLA7Jw7sTvlnu3hu/gYmzSnhe5MX8YPpy4JZexGDDmkbs/baAK4TrlGthugLctuDgG2sP1wV44ejpaVYVKDuaTXkdsz44nn0DDcrI5UHZ67gk52799pWXudM/nPbaUkTwG2NAl1avdyOGVxzcn+uPqkfs9Z9zMQ5pTw9u4Q/vfUeRxXlML64iNHD8nhp2aa4XcSLvgVt/3c/NDxDbujCXHWMAZye2u6L/m1t2HbMTKVX58w6vd3o51+E8l4X5FKadA0iPaVdg/3eW84+TGHeiinQpc0wM47v353j+3dny45Kps7fwNNzSrh5ymLunLaEqhq+CM2yrRXc8txilm7cxtD8zuyod/Gt/gW3nfVbEwdwC1pmWruodkJkppvTIZ2CLpFwrdPnDcJ2rwtyUTPk1vCGq5bo90r8KdClTeqalc6VX+rHt07sy9z3PuHyP8yhuqbuRbzKqhqe/Pf6vX62fVrKXhfVumWlU9S1Q1TYRi7G1T6PDuzan6sN4bB+Tk1z93sl/hTo0qaZGcV9u/L57obvKTbg5e+fHMyC43cLmkhrFNPUwszONrNVZrbGzG5rYP1JZjbfzKrM7OL4lymyf/t6c0ZeTnsG9MjmkM6ZdMxMU5hLqDUa6GaWAjwGnAMMAcab2ZB6w0qAK4Cn412gSCxuPmsQ7evdyhjvN22ItHaxtFyKgTXuvg7AzCYB5wHLawe4+3vButjeSSASZ7qIJxJboOcDpVGvNwCjmrIzM7sKuAqgqKioKZsQ2SddxJNk16KX5939CXcf6e4jc3NzW3LXIiKhF0uglwGFUa8LgmUiItKKxBLoc4GBZtbXzNKBccCM5i1LREQOVKOB7u5VwHXAS8AK4Bl3X2Zm95nZGAAzO8bMNgCXAI+b2bLmLFpERPYW0xuL3H0mMLPesrujns8l0ooREZEECed7lkVEkpACXUQkJBToIiIhoUAXEQkJBbqISEgo0EVEQkKBLiISEgp0EZGQUKCLiISEAl1EJCQU6CIiIaFAFxEJCQW6iEhIKNBFREIipo/PFRGR+GjO77vVDF1EJCQU6CIiIaFAFxEJCQW6iEhI6KKohEpzXnASae00QxcRCQkFuohISKjlIiL7pBZW26IZuohISCjQRURCQi2XBNI/Z0UknjRDFxEJCQW6iEhIKNBFREJCgS4iEhIKdBGRkFCgi4iEhAJdRCQkFOgiIiGhQBcRCQlz98Ts2KwceD8hO4+f7sBHiS6iFdHx2EPHoi4dj7oO5nj0dvfchlYkLNDDwMzmufvIRNfRWuh47KFjUZeOR13NdTzUchERCQkFuohISCjQD84TiS6gldHx2EPHoi4dj7qa5Xiohy4iEhKaoYuIhIQCXUQkJBToMTKzQjN7zcyWm9kyM7shWN7VzF42s9XB/3ZJdK0txcxSzGyBmb0QvO5rZrPNbI2ZTTaz9ETX2FLMLMfMppjZSjNbYWbHJeu5YWbfC/4bWWpmE80sM5nODTP7g5l9aGZLo5Y1eC5YxC+D47LYzI46mH0r0GNXBdzo7kOAY4FrzWwIcBvwirsPBF4JXieLG4AVUa9/DPzM3QcAnwDfSkhVifEL4B/ufhgwnMhxSbpzw8zygeuBke5+BJACjCO5zo0/AWfXW7avc+EcYGDwuAr47UHt2d31aMIDmA58GVgF9AqW9QJWJbq2Fvr9C4IT8zTgBcCIvPMtNVh/HPBSoutsoWPRGVhPcJNB1PKkOzeAfKAU6ErkO4tfAM5KtnMD6AMsbexcAB4Hxjc0rikPzdCbwMz6ACOA2UBPd/8gWLUJ6Jmgslraz4FbgJrgdTdgq7tXBa83EPmPOxn0BcqBPwYtqCfNLIskPDfcvQz4CVACfABsA94hec+NWvs6F2r/ANY6qGOjQD9AZpYNPAd8190/jV7nkT+xob8P1MxGAx+6+zuJrqWVSAWOAn7r7iOAHdRrryTRudEFOI/IH7k8IIu92w9JrTnPBQX6ATCzNCJh/pS7Tw0WbzazXsH6XsCHiaqvBZ0AjDGz94BJRNouvwByzCw1GFMAlCWmvBa3Adjg7rOD11OIBHwynhtnAOvdvdzddwNTiZwvyXpu1NrXuVAGFEaNO6hjo0CPkZkZ8Htghbv/NGrVDODy4PnlRHrroebuE9y9wN37ELng9aq7Xwa8BlwcDEuKYwHg7puAUjMbFCw6HVhOEp4bRFotx5pZh+C/mdpjkZTnRpR9nQszgK8Hd7scC2yLas0cML1TNEZmdiLwb2AJe/rGtxPpoz8DFBH5OOBL3X1LQopMADM7BbjJ3UebWT8iM/auwALga+6+K5H1tRQzO0KBJh0AAABySURBVBJ4EkgH1gHfIDJhSrpzw8zuBcYSuTNsAXAlkb5wUpwbZjYROIXIR+RuBn4ATKOBcyH4o/drIm2pncA33H1ek/etQBcRCQe1XEREQkKBLiISEgp0EZGQUKCLiISEAl1EJCQU6CIiIaFAFxEJif8P1J1PI3t07tQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAeh0lEQVR4nO3de3xU9Z3/8debXCABJSDxBiio1C2tVrrxSre11gtuW7GtFmztgmvVVt31t/amtj/t6q+1rfvYdbdFC11pba2iVYv5dWnZemu3XpAoKkWlIioQvKAQFBMhCZ/9Y050Mk7IhEwy8eT9fDzmwZzv+Z6ZzxxO3ufM95yZUURgZmbpNaTUBZiZWd9y0JuZpZyD3sws5Rz0ZmYp56A3M0s5B72ZWco56M1SStIESSGpvNS1WGk56K1oJD0nqUXSFkmbJP2XpPFFeNyQdECRarwkqa/j1iJpu6QxyfyfSdqW06esi8eaLak9p+8WSXsXo9YCX8/RSf1bJL0uaaWkM3bicb4t6Ya+qNFKz0FvxfbJiBgB7AW8BPywxPV0EhHfjYgRHTfg+8C9EfFKVrcfZPeJiPYdPOQDOX1HRMT63E75jqp7eqS9g/7rk9eyK/AN4CeSJvfksS3dHPTWJyLiTeBW4K3AkTRS0s8lbZD0vKRvSRqSzDtA0h8kbZb0iqSbk/Y/Jos/lhy1zkjaz5K0StJGSfXZR9HJO4AvSXpaUpOkOZKUW2PS9nfA9X2xDpJ3ON+Q9DjwRvIaQ9KZktYAd0sakqyH5yW9nKyfkcnyE3L77+j5ImMhsIms9Z5Vz97JutqYrLuzkvZpwCXAjGQdP1bkVWEl5qC3PiGpGpgBPJjV/ENgJLAf8BEyIdsxzHAF8N/AKGBc0peI+HAy/wPJ0fLNko4BrgQ+S+adw/PAgpwSPgEcChyc9DshT5l/A+wO3JbTfm4Shg9L+kxPXncepwEfB2qAtqTtI8B7k5pmJ7ePklkvI4Af5TxGdv8uJTuNTyXPtTxPlwXAOmBv4BTgu5KOiYjfAd8Fbk7W8Qd69hJtwIsI33wryg14DtgCNAGtwHrgoGReGbANmJzV/xwywyYAPwfmAePyPG4AB2RNX0dmeKVjekTyfBOy+n8oa/4twEV5Hvc64Gc5bR8EdgPKgb8FXgemdvF6Z5MJ76as2zM56+Pvs6YnJLXtl9V2F3Bu1vSByWspz9c/Tw1HA9uT594IPArMzHm+cmA80A7skrXslR2vH/g2cEOptyHf+ubmI3ortpMjogYYBpwP/EHSnsAYoILM0XeH54Gxyf2vAwIekrRC0t/v4Dn2zn6ciNgCvJr1WAAvZt1vJrMzeEvyjuNUcoZtIuKRiHg1ItoiYhHwS+DTO6jlwYioybrtnzN/bZ5lsts6vZbkfjmwRzePkW198tyjI+KQiMh9d9PxPBsj4vWc5xqbp6+ljIPe+kREtEfE7WSOIj8EvELmSHXfrG77AI1J/xcj4qyI2JvMkf41O7jSZn3240gaTuYovLEHJX6KzBHwvd29FDI7oJ2V7+ths9s6vRYy66SNzInsHT1GT60HRkvaJee5OtaZv8Y2xRz01ieUMZ3MmPuTkbly5RbgO5J2kbQvcCFwQ9L/VEnjksU3kQme7cn0S2TGrzvcBJwh6RBJQ8mMLy+JiOd6UOIs4OcR0SngJJ0iaUQy3n08cDpQ34PH7ambgH+SNFHSCN4eK2/rZrkeiYi1wP3AlZKGSToYOJNk/ZNZxxM6To5buvg/1Yrt/0vaArwGfAeYFRErknn/ALwBrAb+BNwIzE/mHQosSZatBy6IiNXJvG8D1ydX0Hw2Iu4E/i+Zk6gvAPsDMwstUNJY4Bgy5wVyXUDmKLcJuAo4KyLu3cHDHZnnOvpDC62FzOv/BfBH4FngTTLrqS+cRmbcfj3wa+CyZF0C/Cr591VJj/TR81uJKOeAxszMUsZH9GZmKeegNzNLOQe9mVnKOejNzFJuwH196ZgxY2LChAmlLsPM7F3l4YcffiUiavPNG3BBP2HCBBoaGkpdhpnZu4qk57ua56EbM7OUc9CbmaWcg97MLOUc9GZmKeegNzNLOQe9mVnKOejNzFLOQW9mlnKpCvoZcx9gxtwHSl2GmdmAkqqgNzOzd3LQm5mlnIPezCzlHPRmZinnoDczSzkHvZlZyjnozcxSzkFvZpZyBQW9pGmSVkpaJemiPPO/JGm5pEcl/UnS5Kx5FyfLrZR0QjGLNzOz7nUb9JLKgDnAicBk4LTsIE/cGBEHRcQhwA+Af02WnQzMBN4HTAOuSR7PzMz6SSFH9IcBqyJidURsAxYA07M7RMRrWZPDgUjuTwcWRMTWiHgWWJU8npmZ9ZNCfhx8LLA2a3odcHhuJ0nnARcClcAxWcs+mLPs2DzLng2cDbDPPvsUUreZmRWoaCdjI2JOROwPfAP4Vg+XnRcRdRFRV1tbW6ySzMyMwoK+ERifNT0uaevKAuDknVzWzMyKrJCgXwpMkjRRUiWZk6v12R0kTcqa/DjwdHK/HpgpaaikicAk4KHel21mZoXqdow+ItoknQ8sBsqA+RGxQtLlQENE1APnSzoWaAU2AbOSZVdIugV4AmgDzouI9j56LWZmlkchJ2OJiEXAopy2S7PuX7CDZb8DfGdnCzQzs97xJ2PNzFLOQW9mlnIOejOzlHPQm5mlnIPezCzlHPRmZinnoDczSzkHvZlZyjnozcxSzkFvZpZyDnozs5Rz0JuZpZyD3sws5Rz0ZmYp56A3M0s5B72ZWco56M3MUs5Bb2aWcg56M7OUc9CbmaWcg97MLOUc9GZmKVdQ0EuaJmmlpFWSLsoz/0JJT0h6XNJdkvbNmtcu6dHkVl/M4s3MrHvl3XWQVAbMAY4D1gFLJdVHxBNZ3ZYBdRHRLOnLwA+AGcm8log4pMh1m5lZgQo5oj8MWBURqyNiG7AAmJ7dISLuiYjmZPJBYFxxyzQzs51VSNCPBdZmTa9L2rpyJvDbrOlhkhokPSjp5HwLSDo76dOwYcOGAkoyM7NCdTt00xOSTgfqgI9kNe8bEY2S9gPulrQ8Ip7JXi4i5gHzAOrq6qKYNZmZDXaFHNE3AuOzpsclbZ1IOhb4JnBSRGztaI+IxuTf1cC9wJRe1GtmZj1USNAvBSZJmiipEpgJdLp6RtIUYC6ZkH85q32UpKHJ/THAVCD7JK6ZmfWxboduIqJN0vnAYqAMmB8RKyRdDjRERD1wFTAC+JUkgDURcRLwXmCupO1kdirfy7lax8zM+lhBY/QRsQhYlNN2adb9Y7tY7n7goN4UaGZmveNPxpqZpZyD3sws5Rz0ZmYp56A3M0s5B72ZWco56M3MUs5Bb2aWcg56M7OUc9CbmaWcg97MLOUc9APQjLkPMGPuA6Uuw8xSwkFvZpZyDnozs5Rz0JuZpZyD3gYFn/ewwcxBb2aWcg56M7OUc9CbmaWcg97MLOUc9GZmKeegNzNLuYKCXtI0SSslrZJ0UZ75F0p6QtLjku6StG/WvFmSnk5us4pZvJmZda/boJdUBswBTgQmA6dJmpzTbRlQFxEHA7cCP0iWHQ1cBhwOHAZcJmlU8co3s1Lw5xKKry/XaSFH9IcBqyJidURsAxYA07M7RMQ9EdGcTD4IjEvunwD8PiI2RsQm4PfAtOKUbmZmhSgk6McCa7Om1yVtXTkT+O1OLmtmZkVWXswHk3Q6UAd8pIfLnQ2cDbDPPvsUsyQzs0GvkCP6RmB81vS4pK0TSccC3wROioitPVk2IuZFRF1E1NXW1hZau5mZFaCQoF8KTJI0UVIlMBOoz+4gaQowl0zIv5w1azFwvKRRyUnY45M2MzPrJ90O3UREm6TzyQR0GTA/IlZIuhxoiIh64CpgBPArSQBrIuKkiNgo6QoyOwuAyyNiY5+8EjMzy6ugMfqIWAQsymm7NOv+sTtYdj4wf2cLNDOz3vEnY83MUs5Bb2aWcg56M7OUc9CbmaVcaoJ+4bJGlq1pYsmzG5n6vbtZuOwdl+ubmQ1KqQj6hcsaufj25Wxr3w5AY1MLF9++3GFvZkZKgv6qxStpaW3v1NbS2s5Vi1eWqCIzs4EjFUG/vqmlR+1mZoNJKoJ+75qqvO177DqsnysxMxt4UhH0XzvhQKoqyt7R3tLaxor1m0tQkZnZwJGKoD95yliu/PRBVJZlXs7Ymiq+esJ7qK4s59QfP8B/r3ixxBWamZVOKoIeMmE/ZZ8aDp84mvsuOobzPzqJO86byqTdR3DODQ8z74/PEBGlLtPMrN+lJujz2X3XYSw4+0j+9v178d1FT3HRbcvZ1ra91GWZmfWrov7C1EBUVVnGD0+bwv61w/mPu1fx3Ktv8OPT/5pRwytLXZqZWb9I9RF9hyFDxIXHH8jVMw5h2ZomPnXNfTyzYUupyzIz6xeDIug7nDxlLDeedTivv9nGp+bcx32rXil1SWZmfW5QBT1A3YTRLDxvKnuOHMas+Q9x45I1pS7JzKxPDbqgBxg/uprbvnwUUw8YwyW/Xs4Vv3mC9u2+IsfM0mlQBj3ALsMquG5WHbOPmsB1f3qWs3/ewJatbaUuy8ys6AZt0AOUlw3h2ye9jyumv497/7KBU669n3WbmktdlplZUQ3qoO/whSMn8NPZh9K4qYWT59zPsjWbSl2SmVnROOgTH35PLbefexTVlWXMmPcg9Y+tL3VJZmZFUVDQS5omaaWkVZIuyjP/w5IekdQm6ZScee2SHk1u9cUqvC9M2mMXFp43lUPG1fCPNy3j6jv/4q9NMLN3vW6DXlIZMAc4EZgMnCZpck63NcBs4MY8D9ESEYckt5N6WW+fGz28kl988TA+88FxXH3n01yw4FHezPlREzOzd5NCvgLhMGBVRKwGkLQAmA480dEhIp5L5qXii2SGlpfxL6cezP67D+cHv1vJ2k3NzPtCHbW7DC11aWZmPVbI0M1YYG3W9LqkrVDDJDVIelDSyfk6SDo76dOwYcOGHjx035HEuUcfwI9P/yBPvvAaJ8+5j6defK3UZZmZ9Vh/nIzdNyLqgM8BV0vaP7dDRMyLiLqIqKutre2Hkgo37f178atzjqJt+3Y+c8393P3US6UuycysRwoJ+kZgfNb0uKStIBHRmPy7GrgXmNKD+gaEg8aN5I7zPsTE2uF88foGrvvTsz5Ja2bvGoUE/VJgkqSJkiqBmUBBV89IGiVpaHJ/DDCVrLH9d5M9Rw7jlnOO5LjJe3DFb57gmwv/TGt7Kk5JmFnKdRv0EdEGnA8sBp4EbomIFZIul3QSgKRDJa0DTgXmSlqRLP5eoEHSY8A9wPci4l0Z9ADVleVc+/m/5stH78+NS9Yw+6cPsbm5tdRlmZntUEE/PBIRi4BFOW2XZt1fSmZIJ3e5+4GDelnjgDJkiPjGtL9ivzHDueTXy/nUtfcxf9ahTBgzvNSlmZnl5U/G7qRT68Zzw5mHs+mNbZx8zX08uPrVUpdkZpaXg74XDt9vNxaeN5XdhlfyheuWcEvD2u4XMjPrZw76Xtp3t+Hcfu5UjthvN75+6+Nc+dsn2e7vtjezAcRBXwQjqyqYP/tQTj9iH+b+YTXn3PAwb/i77c1sgHDQF0lF2RCumP5+vv3Jydz15Euc+uMHeGFzS6nLMjNz0BeTJGZPnch1sw9lzcZmpv/oPh5f11TqssxskHPQ94GPHrg7t335KCrKhvDZuQ+waPkLpS7JzAYxB30fOXDPXbjj/KlM3mtXzv3lI8y5Z5W/NsHMSsJB34fGjBjKjWcdwfRD9uaqxSv5yi2PsbXN321vZv3LQd/HhlWUcfWMQ/jKce/h9mWNfP4nS3h1y9ZSlzWoLFzWyLI1TSx5diNTv3c3C5cV/J18ZqngoO8HkviHj03iR5+bwvLGzZx8zX08/dLrpS5rUFi4rJGLb1/OtuQL6BqbWrj49uUOextUHPT96BMH783N5xxJy7btfPqa+/nDXwbGj6ykUVv7dl7dspXvLnqSlpyfgmxpbef7v3uKdn+wzQaJgr7UzIrnkPE13HH+VL54fQNn/PQhLvvk+5h11IRSlzVgtW8PXn+zlU3NrTQ1b6OpuZWmlm1seqOVppa32zY1b2NzS+bfpuZWXn9zxx9Ye2HzmxzwzUXsOqyCUdUVjKyuZFR1BTVVFdRUV1JTXcGo5N+a6kpqqjLTI6sr2HVYOZL6aQ2Y9Z6DvgTG1lRx65eO5IIFy7isfgXPbNjCpZ+YTHlZet9gRQSvb22j6Y0kqLODO09QNzVvo6mllc0trXR1sZJEp6AePbyS/cYM7xTUV9/5Fzbl+SrpkVXlzDpqYqcdxcY3tvHMhi3d7ijKhoiRVR07hbd3DDVVyc6iuvPOYmRVBaOGVzK8ssw7CCsJB32JDB9aztwv1PG93z7JT/7nWZ57tZkffW4Kuw6rKHVpOxQRNG9rzwrkt4N7c3NHgL8d1Juat7G5OXP0vaOhkl2GllMzPBOWNdUVjB9dnfcIe2THkXZVBbtWVVA2ZMfBObKqgotvX95p+Kaqoox/Pun9nDyl658+bmvfnux4WtncxTuIppZWNje38tJrb7Lyxddpat7GG9u6vqqqokyMrOp4LRWM7GLHkPuah1UM8Q7CesVBX0JlQ8Q3Pz6Z/WtH8K2Ff+Yz19zPdbMO7bfnf7P17cDuCORNSXB3hHUmwN8OtqbmbbS2dx3Y1ZVlWUexFbx3z12TIMs6us0eEqmuYGRVBRV99G6mI8y/fuvjbGvfztiaKr52woE7DHmA8rIh7DZiKLuNGNqj59vWtp2mlqx1mT3clL0TbG5l3aZmVqzPrNs3W7v+tbLK8iHJTq+y87rsYsfQ0WdoeVmParf00kD7EE9dXV00NDTs1LIz5j4AwM3nHFnMkvrF/c+8wpdveIS29u1sbdtO2/YoOJS2trV3Cpa3jkJzgqVjeKTj/ta2rsNlaPmQTqFRU1XJqOF5jkKTYYmaqgpGDuBwGejbxput7Z3OP3T1/5f9rqKpufWtq4nyqaoo63z+Ic/5hlHVnd9h1FR3v9NduKyxxztO27FirFNJD0dEXb55PqIfII7afwznfXR/rlz0FB273samFr5262Pc89TLjBtd1fnoOmt4pLmb4YLsP+7xo6s5eNw7x5U7/9FXMqxiYAZ2Wg2rKGPPkWXsOXJYwctEBC2t7V2f73ij845h5Yuvv7Wjb9vBMNqIoeVdvgtbu7GZ/1r+wlvv6hqbWvjGbY/z3Ktv8OH31PZ6PQxGf/zLBq6995l3XAIMFG0H6qAfQK6//3ly//xa24M7HltP2RC9ddQ8qrqSvUYO47177fr20dhbV410fntf7ROAqSWJ6spyqivLGVtTVfByEcGWrW2ddgzvuIIpq33dphaakp1Hvv3D1rbtXH3n01x959NFfHWDW0trO1ctXumgT6P1Tfm/1ljA0//vRIZ0c+LRrBCS2GVYBbsMq2D86MKX27492P+SRe84GOnwszP67/xSmsz+6dK87V3lwc5w0A8ge9dU0ZjnP3fvmiqHvJXckCHqchsdW1PF0QfuXoKq3v3G7uDvvlgKutRB0jRJKyWtknRRnvkflvSIpDZJp+TMmyXp6eQ2q1iFp9HXTjiQqpyx8aqKMr52woElqsisM2+jxdcf67TbI3pJZcAc4DhgHbBUUn1EPJHVbQ0wG/hqzrKjgcuAOiCAh5NlNxWn/HTZ2UsBzfqLt9Hi6491WsjQzWHAqohYDSBpATAdeCvoI+K5ZF7utV4nAL+PiI3J/N8D04Cbel15Sp08ZSw3PbQGGLiXAtrg5m20+Pp6nRYydDMWWJs1vS5pK0RvljUzsyIYEF+uIulsSQ2SGjZs8Dc6mpkVUyFB3wiMz5oel7QVoqBlI2JeRNRFRF1trT90YWZWTIUE/VJgkqSJkiqBmUB9gY+/GDhe0ihJo4DjkzYzM+sn3QZ9RLQB55MJ6CeBWyJihaTLJZ0EIOlQSeuAU4G5klYky24EriCzs1gKXN5xYtbMzPpHQR+YiohFwKKctkuz7i8lMyyTb9n5wPxe1GhmZr0wIE7GmplZ33HQm5mlnIPezCzlHPRmZinnoDczSzkHvZlZyjnozcxSzkFvZpZyDnozs5Rz0JuZpZyD3sws5Rz0ZmYp56A3M0s5B72ZWco56M3MUs5Bb2aWcg56M7OUc9CbmaWcg97MLOUc9GZmKeegNzNLOQe9mVnKFRT0kqZJWilplaSL8swfKunmZP4SSROS9gmSWiQ9mtx+XNzyzcysO+XddZBUBswBjgPWAUsl1UfEE1ndzgQ2RcQBkmYC3wdmJPOeiYhDily3mZkVqJAj+sOAVRGxOiK2AQuA6Tl9pgPXJ/dvBT4mScUr08zMdlYhQT8WWJs1vS5py9snItqAzcBuybyJkpZJ+oOkv8n3BJLOltQgqWHDhg09egFmZrZjfX0y9gVgn4iYAlwI3Chp19xOETEvIuoioq62traPSzIzG1wKCfpGYHzW9LikLW8fSeXASODViNgaEa8CRMTDwDPAe3pbtJmZFa6QoF8KTJI0UVIlMBOoz+lTD8xK7p8C3B0RIak2OZmLpP2AScDq4pRuZmaF6Paqm4hok3Q+sBgoA+ZHxApJlwMNEVEPXAf8QtIqYCOZnQHAh4HLJbUC24EvRcTGvnghZmaWX7dBDxARi4BFOW2XZt1/Ezg1z3K3Abf1skYzM+uFgoL+3eLmc44sdQlmZgOOvwLBzCzlHPRmZinnoDczSzkHvZlZyjnozcxSzkFvZpZyDnozs5Rz0JuZpZyD3sws5Rz0ZmYpl6qvQDDrir8ewwYzH9GbmaWcg97MLOUc9GZmKecxejPrMZ/zKL6+XKc+ojczSzkHvZlZyjnozcxSzkFvZpZyDnozs5Rz0JuZpVxBQS9pmqSVklZJuijP/KGSbk7mL5E0IWvexUn7SkknFK90MzMrRLdBL6kMmAOcCEwGTpM0OafbmcCmiDgA+Dfg+8myk4GZwPuAacA1yeOZmVk/KeSI/jBgVUSsjohtwAJgek6f6cD1yf1bgY9JUtK+ICK2RsSzwKrk8czMrJ8UEvRjgbVZ0+uStrx9IqIN2AzsVuCySDpbUoOkhg0bNhRevZmZdWtAnIyNiHkRURcRdbW1taUux8wsVQr5rptGYHzW9LikLV+fdZLKgZHAqwUuazn8PSJmVkyFHNEvBSZJmiipkszJ1fqcPvXArOT+KcDdERFJ+8zkqpyJwCTgoeKUbmZmhej2iD4i2iSdDywGyoD5EbFC0uVAQ0TUA9cBv5C0CthIZmdA0u8W4AmgDTgvItr76LWYmVkeyhx4Dxx1dXXR0NBQ6jLMzN5VJD0cEXX55g2Ik7FmZtZ3HPRmZinnoDczSzkHvZlZyjnozcxSzkFvZpZyA+7ySkkbgOdLXUcvjQFeKXURA4jXR2deH2/zuuisN+tj34jI+x0yAy7o00BSQ1fXsw5GXh+deX28zeuis75aHx66MTNLOQe9mVnKOej7xrxSFzDAeH105vXxNq+LzvpkfXiM3sws5XxEb2aWcg56M7OUc9D3kqTxku6R9ISkFZIuSNpHS/q9pKeTf0eVutb+IqlM0jJJv0mmJ0paImmVpJuTH7AZFCTVSLpV0lOSnpR05CDfNv4p+Tv5s6SbJA0bTNuHpPmSXpb056y2vNuDMv4jWS+PS/rgzj6vg7732oCvRMRk4AjgPEmTgYuAuyJiEnBXMj1YXAA8mTX9feDfIuIAYBNwZkmqKo1/B34XEX8FfIDMehmU24akscA/AnUR8X4yP2Q0k8G1ffwMmJbT1tX2cCKZX+WbBJwNXLvTzxoRvhXxBtwBHAesBPZK2vYCVpa6tn56/eOSjfUY4DeAyHzSrzyZfySwuNR19tO6GAk8S3LRQ1b7YN02xgJrgdFkft3uN8AJg237ACYAf+5uewDmAqfl69fTm4/oi0jSBGAKsATYIyJeSGa9COxRorL629XA14HtyfRuQFNEtCXT68j8wQ8GE4ENwE+Toaz/lDScQbptREQj8C/AGuAFYDPwMIN3++jQ1fbQsWPssNPrxkFfJJJGALcB/yciXsueF5ndceqvY5X0CeDliHi41LUMEOXAB4FrI2IK8AY5wzSDZdsASMaep5PZAe4NDOedwxiDWl9tDw76IpBUQSbkfxkRtyfNL0naK5m/F/ByqerrR1OBkyQ9BywgM3zz70CNpI4foh8HNJamvH63DlgXEUuS6VvJBP9g3DYAjgWejYgNEdEK3E5mmxms20eHrraHRmB8Vr+dXjcO+l6SJOA64MmI+NesWfXArOT+LDJj96kWERdHxLiImEDmJNvdEfF54B7glKTboFgXABHxIrBW0oFJ08eAJxiE20ZiDXCEpOrk76ZjfQzK7SNLV9tDPfB3ydU3RwCbs4Z4esSfjO0lSR8C/gdYztvj0peQGae/BdiHzNcufzYiNpakyBKQdDTw1Yj4hKT9yBzhjwaWAadHxNZS1tdfJB0C/CdQCawGziBzgDUotw1J/wzMIHO12jLgi2TGnQfF9iHpJuBoMl9H/BJwGbCQPNtDsjP8EZnhrWbgjIho2KnnddCbmaWbh27MzFLOQW9mlnIOejOzlHPQm5mlnIPezCzlHPRmZinnoDczS7n/Bel5YbtLu/5UAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}