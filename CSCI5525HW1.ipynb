{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSCI5525HW1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hvH6TaH4J1XD",
        "OqRI5c4vM3Dj",
        "pfTwRvgYJOY1",
        "pewIo2Y1lzvf"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOwVcUmKhjvtKITP8zeVV+C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TonyLiu836/CSCI5525-MachineLearning/blob/main/CSCI5525HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CSCI5525 HW 1\n",
        "Tony Liu - ID: 5383942"
      ],
      "metadata": {
        "id": "6nyGyljKlWTh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyGXGnjGlTcT",
        "outputId": "d4ac4619-e789-43eb-cbc1-e756e7f39e71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets \n",
        "import math\n",
        "\n",
        "#change path for datasets\n",
        "\n",
        "boston_data_path = \"/content/drive/MyDrive/Grad School/Spring2022/CSCI5525 ML/HW1/boston.csv\"        \n",
        "digits_data_path = \"/content/drive/MyDrive/Grad School/Spring2022/CSCI5525 ML/HW1/digits.csv\"\n",
        "#boston_data_path = \"C:\\\\GradSchool\\\\Spring2022\\\\CSCI5525ML\\\\HW1\\\\boston.csv\"\n",
        "#digits_data_path = \"C:\\\\GradSchool\\\\Spring2022\\\\CSCI5525ML\\\\HW1\\\\digits.csv\"\n"
      ],
      "metadata": {
        "id": "M2WogBR5L16O"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def getBostonDataset(tau):\n",
        "    boston = np.loadtxt(open(boston_data_path, \"rb\"), delimiter=\",\", skiprows=1)\n",
        "    boston = np.asarray(boston)\n",
        "    #X = boston[:,:-1]\n",
        "    y = boston[:,-1:]\n",
        "    boundary_value = np.percentile(y, tau)      #split boston dataset into either 50 or 75 \n",
        "    for i in range(len(y)):\n",
        "        if y[i] >= boundary_value:\n",
        "            boston[i][-1] = 1\n",
        "        else:\n",
        "            boston[i][-1] = 0\n",
        "    #return X, y\n",
        "    return boston\n",
        "\n",
        "def getDigitsDataset():\n",
        "    digits = np.loadtxt(open(digits_data_path, \"rb\"), delimiter=\",\", skiprows=1)\n",
        "    #data = datasets.load_digits()\n",
        "    return digits\n",
        "\n",
        "\n",
        "def chooseDataset(dataset):\n",
        "    if dataset == \"Boston50\":\n",
        "        boston50 = getBostonDataset(50)\n",
        "        return boston50\n",
        "\n",
        "    elif dataset == \"Boston75\":\n",
        "        boston75= getBostonDataset(75)\n",
        "        return boston75\n",
        "\n",
        "    else:\n",
        "        digits = getDigitsDataset()\n",
        "        return digits\n",
        "\n"
      ],
      "metadata": {
        "id": "U20sjL1YmEjG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA1dThres"
      ],
      "metadata": {
        "id": "hvH6TaH4J1XD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randrange\n",
        "from collections import defaultdict\n",
        "#split data into 10-folds\n",
        "def crossValSplit(dataset, folds = 10):\n",
        "    fold_size = int(len(dataset) / folds)\n",
        "    split_dataset = []\n",
        "    dataset_copy = dataset\n",
        "    for i in range(folds):\n",
        "        each_fold = []\n",
        "        while len(each_fold) < fold_size:\n",
        "            randSample = randrange(len(dataset_copy[0]))\n",
        "\n",
        "            each_fold.append(dataset_copy[randSample])\n",
        "            np.delete(dataset_copy,randSample)\n",
        "        split_dataset.append(each_fold)\n",
        "    \n",
        "    return split_dataset \n",
        "\n",
        "#seperate samples into classes based on their labels\n",
        "def seperateData(data, labels):\n",
        "    class1 = []       #y=1 class\n",
        "    class0 = []       #y=0 class\n",
        "    for i in range(np.shape(data)[0]):\n",
        "\n",
        "        if labels[i] == 1:\n",
        "            class1.append(data[i])\n",
        "        else:\n",
        "            class0.append(data[i])\n",
        "    return class0, class1\n",
        "\n",
        "def softmax(vect):\n",
        "    e = np.exp(vect)\n",
        "    return e / e.sum()\n",
        "\n",
        "\n",
        "#get mean of features in each class\n",
        "def calcMeans(class0, class1):\n",
        "    #print(np.shape(class0))\n",
        "    class1Mean = np.mean(class1, axis = 0)      \n",
        "    class0Mean = np.mean(class0, axis = 0)      \n",
        "    return class0Mean, class1Mean\n",
        "\n",
        "#finds the between-class scatter(SB) and within-class scatter(SW)\n",
        "def calcCovariances(class0, class1, class0Mean, class1Mean):\n",
        "    diffMeans = np.asarray(class1Mean - class0Mean)\n",
        "    SB = np.matmul(np.transpose([diffMeans]), [diffMeans])\n",
        "    length = len(class0[0])\n",
        "    sum0 = 0\n",
        "    sum1 = 0\n",
        "    for n in range(len(class0Mean)):\n",
        "        temp0 = class0[n] - class0Mean\n",
        "        sum0 += np.matmul(np.transpose([temp0]), [temp0])\n",
        "\n",
        "    for n in range(len(class1Mean)):  \n",
        "        temp1 = class1[n] - class1Mean\n",
        "        sum1 += np.matmul(np.transpose([temp1]), [temp1])\n",
        "\n",
        "    SW = sum0 + sum1\n",
        "\n",
        "    return SB, SW\n",
        "\n",
        "\n",
        "# projects samples data @ W.T\n",
        "def projectData(W, data):\n",
        "    #projectedPts = []\n",
        "    #print(np.shape(data))\n",
        "    #print(np.shape(W))\n",
        "    projectedPts = data @ W\n",
        "\n",
        "    return projectedPts\n",
        "\n",
        "\n",
        "\n",
        "# takes in samples of both classes and then finds the best boundary to classify samples\n",
        "def trainLDA1D(train_data, train_labels):\n",
        "    class0, class1 = seperateData(train_data,train_labels)\n",
        "    class0Mean, class1Mean = calcMeans(class0, class1)\n",
        "    SB,SW = calcCovariances(class0,class1, class0Mean, class1Mean)\n",
        "\n",
        "    #eigVal, eigVect = np.linalg.eig(np.matmul(np.linalg.pinv(SW),SB))     #find the largest eigenvalue and the corresponding eigenvector\n",
        "    #largestEigValInd = np.where(eigVal == max(eigVal))\n",
        "    #W = eigVect[largestEigValInd][0]                                    #set W to be the previously found eigenvector\n",
        "\n",
        "    eigVal, eigVect = np.linalg.eig(np.linalg.pinv(SW) @ SB)\n",
        "    eigPairs = [(abs(eigVal[i]), eigVect[:,i]) for i in range(len(eigVal))]       #combine the eigval & eigvect into pairs\n",
        "    sortedEigPairs = sorted(eigPairs, key = lambda x: x[0], reverse = True)       #sort the eigen pairs in descending order \n",
        "    W = np.transpose([sortedEigPairs[0][1]])\n",
        "    #print(W)\n",
        "  \n",
        "    projPtsC0 = projectData(W, class0)\n",
        "    projPtsC1 = projectData(W, class1)\n",
        "    \n",
        "    projC0Mean, projC1Mean = calcMeans(projPtsC0, projPtsC1)            #assume P(x|Ci) are Gaussian and calc mean, cov, priors\n",
        "    projMeans = [projC0Mean, projC1Mean]\n",
        "\n",
        "    projC0Cov = np.cov(projPtsC0, rowvar = False)\n",
        "    projC1Cov = np.cov(projPtsC1, rowvar = False)\n",
        "    projCovs = [projC0Cov, projC1Cov]\n",
        "\n",
        "    priorC1 = len(class0)/(len(class0) + len(class1))\n",
        "    priorC2 = len(class1)/ (len(class0) + len(class1))\n",
        "    priors = [priorC1, priorC2]\n",
        "\n",
        "    return W, projMeans, projCovs, priors\n",
        "\n",
        "def predict(data, W, means, covs, priors):\n",
        "    y_hat = []\n",
        "    proj = projectData(W, data)  \n",
        "    proj = np.asarray(proj)\n",
        "    for sample in proj:\n",
        "        sample_pred = []\n",
        "        for i in range(len(means)):                     #use the un-simplified discriminant form \n",
        "            #invCov = np.linalg.inv(covs[i])\n",
        "            t1 = -(0.5) * np.log(2*math.pi)\n",
        "            t2 = np.log(covs[i])\n",
        "            t3 = ((sample - means[i])**2) / (2*covs[i]**2)\n",
        "            t4 = np.log(priors[i])\n",
        "            g = t1 - t2 - t3 + t4\n",
        "            sample_pred.append(g)\n",
        "        pred = softmax(sample_pred)\n",
        "        y_hat.append(np.argmax(pred))\n",
        "    \n",
        "    return y_hat\n",
        "\n",
        "\n",
        "def errorAndStdDev(prediction, gndTruth):\n",
        "    counter = 0\n",
        "    temp = 0\n",
        "    gndTruthMean = sum(gndTruth) / len(gndTruth)\n",
        "    for i in range(len(prediction)):\n",
        "        #print(prediction[i], gndTruth[i])\n",
        "        if prediction[i] != gndTruth[i]:\n",
        "            counter += 1\n",
        "    \n",
        "    error = counter / len(prediction) * 100\n",
        "    \n",
        "    mean = np.mean(gndTruth)\n",
        "    for i in range(len(prediction)):\n",
        "        temp += (prediction[i] - mean)**2\n",
        "    stdDev = np.sqrt(temp/len(gndTruth)) / np.sqrt(len(prediction)) *100 \n",
        "    return error, stdDev\n",
        "\n",
        "def LDA1dThres(filename, num_crossval):#, prior):\n",
        "    split_data = crossValSplit(filename)\n",
        "    \n",
        "    globalError = 0\n",
        "    globalStdDev = 0\n",
        "\n",
        "    fold_error = []\n",
        "    fold_stddev = []\n",
        "    for i in range(len(split_data)):\n",
        "        #print(len(split_data[i]))\n",
        "        test_data = np.copy(split_data[i])\n",
        "        test_labels = test_data[:,-1]\n",
        "        test_data = test_data[:,:-1]\n",
        "\n",
        "        train_data_folds = np.delete(np.copy(split_data),i,0)\n",
        "        folds, samples, features = np.shape(train_data_folds)\n",
        "        train_data = np.reshape(train_data_folds, (folds * samples, features))\n",
        "        \n",
        "        train_labels = train_data[:,-1]\n",
        "\n",
        "        train_data = train_data[:,:-1]\n",
        "\n",
        "        class0, class1 = seperateData(train_data, train_labels)\n",
        "        W, means,covs, priors = trainLDA1D(train_data, train_labels)\n",
        "        trainPred = predict(train_data, W, means, covs, priors)\n",
        "        trainError, trainStdDev = errorAndStdDev(trainPred, train_labels)\n",
        "\n",
        "        prediction = predict(test_data, W, means, covs, priors)\n",
        "        error, stdDev= errorAndStdDev(prediction, test_labels)\n",
        "        #fold_error.append(error)\n",
        "        #fold_stddev.append(stdDev)\n",
        "        globalStdDev += stdDev\n",
        "        globalError += error\n",
        "        print(\"Fold= \", i, \"  Train error= \", trainError, \"%  Train std dev= \", trainStdDev, \"  Test Error\", error, \"%  Test std dev= \", stdDev)\n",
        "    \n",
        "    \n",
        "    print(\"Avg Error = \", globalError / num_crossval, \"%\")\n",
        "    print(\"Avg Standard Deviation = \", globalStdDev / num_crossval)\n",
        "\n",
        "#dataset, prior = chooseDataset(\"Boston50\")\n",
        "#print(\"dataset=\", dataset)\n",
        "dataset= chooseDataset(\"Boston50\")\n",
        "LDA1dThres(dataset, 10)#, prior)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_H5mfK16iOt",
        "outputId": "f72714f4-0b0f-47c9-849e-92fd7ad2a126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in true_divide\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold=  0   Train error=  26.666666666666668 %  Train std dev=  2.434322477800744   Test Error 22.0 %  Test std dev=  7.0710678118654755\n",
            "Fold=  1   Train error=  33.77777777777778 %  Train std dev=  2.418039201552962   Test Error 48.0 %  Test std dev=  8.376156636548767\n",
            "Fold=  2   Train error=  28.000000000000004 %  Train std dev=  2.4179484326120124   Test Error 30.0 %  Test std dev=  7.63675323681471\n",
            "Fold=  3   Train error=  27.555555555555557 %  Train std dev=  2.432089991837274   Test Error 28.000000000000004 %  Test std dev=  7.222188034107117\n",
            "Fold=  4   Train error=  28.666666666666668 %  Train std dev=  2.4352464433922987   Test Error 24.0 %  Test std dev=  7.200000000000002\n",
            "Fold=  5   Train error=  28.22222222222222 %  Train std dev=  2.4390512896238294   Test Error 22.0 %  Test std dev=  7.0710678118654755\n",
            "Fold=  6   Train error=  27.555555555555557 %  Train std dev=  2.4270989530396907   Test Error 28.000000000000004 %  Test std dev=  7.359347797189642\n",
            "Fold=  7   Train error=  27.111111111111114 %  Train std dev=  2.4356745037188103   Test Error 32.0 %  Test std dev=  7.0710678118654755\n",
            "Fold=  8   Train error=  28.666666666666668 %  Train std dev=  2.4404006956964226   Test Error 24.0 %  Test std dev=  7.0710678118654755\n",
            "Fold=  9   Train error=  28.000000000000004 %  Train std dev=  2.4333530637905705   Test Error 30.0 %  Test std dev=  7.233256528010052\n",
            "Avg Error =  28.8 %\n",
            "Avg Standard Deviation =  7.331197348013218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA2dGaussGM"
      ],
      "metadata": {
        "id": "OqRI5c4vM3Dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randrange\n",
        "\n",
        "\n",
        "#split data into 10-folds\n",
        "def crossValSplit(dataset, folds = 10):\n",
        "    fold_size = int(len(dataset) / folds)\n",
        "    split_dataset = []\n",
        "    dataset_copy = dataset\n",
        "    for i in range(folds):\n",
        "        each_fold = []\n",
        "        while len(each_fold) < fold_size:\n",
        "            randSample = randrange(len(dataset_copy[0]))\n",
        "            each_fold.append(dataset_copy[randSample])\n",
        "            np.delete(dataset_copy,randSample)\n",
        "        split_dataset.append(each_fold)\n",
        "    \n",
        "    return split_dataset \n",
        "\n",
        "#seperate samples into classes based on their labels\n",
        "def seperateData(data, labels):\n",
        "    data_dict = {0:[],              #use dictionary to store samples of different classes\n",
        "                1:[],\n",
        "                2:[],\n",
        "                3:[],\n",
        "                4:[],\n",
        "                5:[],\n",
        "                6:[],\n",
        "                7:[],\n",
        "                8:[],\n",
        "                9:[]}\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        data_dict[labels[i]].append(data[i,:])\n",
        "    return data_dict\n",
        "\n",
        "def calcMeans(data_dict):\n",
        "    class_means = []                       #list of means of features for samples of each class\n",
        "    for value in data_dict.values():\n",
        "        mean = np.mean(value, axis = 0)\n",
        "        class_means.append(mean) \n",
        "    temp = sum(class_means)                              #find overall mean \n",
        "    overall_mean = (1/len(class_means)) * temp\n",
        "\n",
        "    return class_means, overall_mean\n",
        "\n",
        "\n",
        "#finds the between-class scatter and within-class scatter\n",
        "def calcCovariances(data_dict, class_means, overall_mean):\n",
        "    num_classes = len(class_means)\n",
        "\n",
        "    Si = []\n",
        "    counter1 = 0\n",
        "    for key, value in data_dict.items():\n",
        "        for i in range(len(value)):\n",
        "            diff = value[i] - class_means[key]\n",
        "            counter1 += np.matmul(np.transpose([diff]), [diff])\n",
        "        Si.append(counter1)\n",
        "\n",
        "    SW = sum(Si)                        #within class scatter\n",
        "\n",
        "    SB = 0                              #between class scatter\n",
        "    for i in range(num_classes):\n",
        "        diffMeans = class_means[i] - overall_mean\n",
        "        SB += len(data_dict[i]) * np.matmul(np.transpose([diffMeans]), [diffMeans])\n",
        "    \n",
        "    return SB, SW\n",
        "\n",
        "# projects samples by multiplying W.T and each sample \n",
        "def projectData(W, data):\n",
        "    projectedData = []\n",
        "    for i in range(len(data)):\n",
        "        projectedData.append(data[i] @ np.transpose(W))\n",
        "    return projectedData\n",
        "\n",
        "\n",
        "def findGaussParams(train_data, labels ,W):\n",
        "    means = {}\n",
        "    covs = {}\n",
        "    priors = {}\n",
        "\n",
        "    data_dict = seperateData(train_data, labels)\n",
        "    samples = len(train_data)\n",
        "    for key, value in data_dict.items():\n",
        "        \n",
        "        projection = projectData(W, value)\n",
        "        means[key] = np.mean(projection, axis = 0)\n",
        "        covs[key] = np.cov(projection, rowvar = False)\n",
        "        priors[key] = len(data_dict[key])/samples\n",
        "    return means,covs,priors\n",
        "\n",
        "\n",
        "def Gaussian(u, sigma, sample):\n",
        "    d = len(sample)\n",
        "    #smallVals = 1e-15*np.eye(np.shape(sigma)[0])\n",
        "    #sigma = sigma + smallVals\n",
        "    temp1 = (2*math.pi)**(d/2) * (np.linalg.det(sigma)**(1/2))\n",
        "    temp2 = (-1/2) * (sample-u) @ np.linalg.inv(sigma) @ np.transpose([sample-u])\n",
        "    gauss = (1/temp1) * np.exp(temp2)\n",
        "    return gauss\n",
        "\n",
        "\n",
        "# takes in samples of both classes and then finds the best boundary to classify samples\n",
        "def train(train_data, labels, test_data):\n",
        "\n",
        "    data_dict = seperateData(train_data, labels)\n",
        "    class_means, overall_mean = calcMeans(data_dict)\n",
        "    SB,SW = calcCovariances(data_dict, class_means, overall_mean)\n",
        "    eigVal, eigVect = np.linalg.eig(np.matmul(np.linalg.pinv(SW), SB))\n",
        "\n",
        "    idx = eigVal.argsort()[::-1]\n",
        "    k=2\n",
        "    idx = eigVal.argsort()[-k:][::-1]\n",
        "    W = eigVect[idx]\n",
        "   \n",
        "    u, sigma, priors = findGaussParams(train_data, labels, W)\n",
        "    return W,u,sigma,priors \n",
        "\n",
        "def predict(data,W,u,sigma,priors):\n",
        "    y_hat = []\n",
        "    proj = projectData(W, data)\n",
        "    for sample in proj:\n",
        "        probs = []\n",
        "        for c in range(len(sigma)):        #for each class, find gaussian and multiply w prior\n",
        "            gauss = Gaussian(u[c], sigma[c], sample)\n",
        "\n",
        "            prob = priors[c] * gauss\n",
        "            probs.append(prob)\n",
        "        \n",
        "        y_hat.append(probs)\n",
        "    predictions = np.argmax(y_hat, axis=1)\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "def errorAndStdDev(y_hat, gndTruth):\n",
        "    counter = 0\n",
        "    temp = 0\n",
        "    gndTruthMean = sum(gndTruth) / len(gndTruth)\n",
        "    for i in range(len(y_hat)):\n",
        "        if y_hat[i] != gndTruth[i]:\n",
        "            counter += 1\n",
        "    \n",
        "    error = counter / len(y_hat)\n",
        "    \n",
        "    for i in range(len(gndTruth)):\n",
        "        temp +=  (y_hat[i] - gndTruthMean)**2\n",
        "    stdDev = np.sqrt(temp/len(gndTruth))\n",
        "    return error, stdDev\n",
        "\n",
        "\n",
        "def LDA2dGaussGM(filename, num_crossval):\n",
        "    split_data = crossValSplit(filename)\n",
        "    \n",
        "    globalError = 0\n",
        "    globalStdDev = 0\n",
        "\n",
        "    fold_error = []\n",
        "    fold_stddev = []\n",
        "    for i in range(len(split_data)):\n",
        "        test_data = np.copy(split_data[i])\n",
        "        test_data_labels = test_data[:,-1]\n",
        "        test_data = test_data[:,:-1]\n",
        "        \n",
        "        train_data_folds = np.delete(np.copy(split_data),i,0)\n",
        "        folds, samples, features = np.shape(train_data_folds)\n",
        "        train_data = np.reshape(train_data_folds, (folds * samples, features))\n",
        "        train_data_labels = train_data[:,-1]\n",
        "        train_data = train_data[:,:-1]\n",
        "        \n",
        "        W,u,sigma,priors = train(train_data, train_data_labels, test_data)\n",
        "        \n",
        "        trainPred = predict(train_data,W,u,sigma,priors)\n",
        "        trainError, trainStdDev = errorAndStdDev(trainPred, train_data_labels)\n",
        "        \n",
        "        testPred = predict(test_data, W, u, sigma, priors)\n",
        "        testError, testStdDev = errorAndStdDev(testPred,test_data_labels)\n",
        "\n",
        "        print(\"Fold= \", i, \"  Train error= \", trainError * 100, \"%  Train std dev= \", trainStdDev, \"  Test Error\", testError * 100, \"%   Test std dev= \", testStdDev)\n",
        "        globalError += trainError \n",
        "        globalStdDev += trainStdDev\n",
        "    \n",
        "    print(\"Avg Error = \", globalError / num_crossval * 100, \"%\")\n",
        "    print(\"Avg Standard Deviation = \", globalStdDev / num_crossval)\n",
        "\n",
        "dataset = chooseDataset(\"digits\")\n",
        "LDA2dGaussGM(dataset, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-NLsxBcBj08",
        "outputId": "5fcce71b-c105-4e63-8b93-a74df81c4b09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold=  0   Train error=  49.534450651769085 %  Train std dev=  [3.0674529]   Test Error 53.63128491620112 %   Test std dev=  [3.05378684]\n",
            "Fold=  1   Train error=  59.40409683426443 %  Train std dev=  [2.86782429]   Test Error 67.0391061452514 %   Test std dev=  [2.8089061]\n",
            "Fold=  2   Train error=  63.00434512725015 %  Train std dev=  [2.70585035]   Test Error 63.128491620111724 %   Test std dev=  [2.71245016]\n",
            "Fold=  3   Train error=  52.6381129733085 %  Train std dev=  [2.98181592]   Test Error 45.81005586592179 %   Test std dev=  [2.94029941]\n",
            "Fold=  4   Train error=  51.21042830540037 %  Train std dev=  [2.85189139]   Test Error 50.27932960893855 %   Test std dev=  [2.81863369]\n",
            "Fold=  5   Train error=  35.071384233395406 %  Train std dev=  [2.93270766]   Test Error 39.10614525139665 %   Test std dev=  [2.93533822]\n",
            "Fold=  6   Train error=  65.67349472377406 %  Train std dev=  [2.85706829]   Test Error 68.71508379888269 %   Test std dev=  [2.86044976]\n",
            "Fold=  7   Train error=  71.13594040968343 %  Train std dev=  [2.92475068]   Test Error 72.06703910614524 %   Test std dev=  [2.84224706]\n",
            "Fold=  8   Train error=  40.53382991930478 %  Train std dev=  [2.81679145]   Test Error 40.22346368715084 %   Test std dev=  [2.87947507]\n",
            "Fold=  9   Train error=  50.65176908752328 %  Train std dev=  [2.76481445]   Test Error 54.7486033519553 %   Test std dev=  [2.76985697]\n",
            "Avg Error =  53.88578522656735 %\n",
            "Avg Standard Deviation =  [2.87709674]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA2dGaussianGM(New) 100% accuracy"
      ],
      "metadata": {
        "id": "pfTwRvgYJOY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randrange\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#split data into 10-folds\n",
        "def crossValSplit(dataset, folds = 10):\n",
        "    fold_size = int(len(dataset) / folds)\n",
        "    split_dataset = []\n",
        "    dataset_copy = dataset\n",
        "    for i in range(folds):\n",
        "        each_fold = []\n",
        "        while len(each_fold) < fold_size:\n",
        "            randSample = randrange(len(dataset_copy[0]))\n",
        "            each_fold.append(dataset_copy[randSample])\n",
        "            np.delete(dataset_copy,randSample)\n",
        "        split_dataset.append(each_fold)\n",
        "    \n",
        "    return split_dataset\n",
        "    \n",
        "def errorAndStdDev(y_hat, gndTruth):\n",
        "    counter = 0\n",
        "    temp = 0\n",
        "    gndTruthMean = sum(gndTruth) / len(gndTruth)\n",
        "\n",
        "    for i in range(len(y_hat)):\n",
        "        print(y_hat[i], \"  \", gndTruth[i])\n",
        "        if y_hat[i] != gndTruth[i]:\n",
        "            counter += 1\n",
        "    \n",
        "    error = counter / len(y_hat)\n",
        "\n",
        "    for i in range(len(gndTruth)):\n",
        "        temp +=  (y_hat[i] - gndTruthMean)**2\n",
        "    stdDev = np.sqrt(temp/len(gndTruth))\n",
        "    return error, stdDev\n",
        "\n",
        "\n",
        "#seperate samples into classes based on their labels\n",
        "def seperateData(data, labels):\n",
        "    data_dict = {0:[],              #use dictionary to store samples of different classes\n",
        "                1:[],\n",
        "                2:[],\n",
        "                3:[],\n",
        "                4:[],\n",
        "                5:[],\n",
        "                6:[],\n",
        "                7:[],\n",
        "                8:[],\n",
        "                9:[]}\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        data_dict[labels[i]].append(data[i,:])\n",
        "    return data_dict\n",
        "\n",
        "#list of means of features for samples of each class and overall mean\n",
        "def calcMeans(data_dict):\n",
        "    class_means = []                       \n",
        "    for value in data_dict.values():\n",
        "        mean = np.mean(value, axis = 0)\n",
        "        class_means.append(mean) \n",
        "    temp = sum(class_means)                              \n",
        "    overall_mean = (1/len(class_means)) * temp\n",
        "\n",
        "    return class_means, overall_mean\n",
        "\n",
        "\n",
        "#finds the between-class scatter and within-class scatter\n",
        "def calcCovariances(data_dict, class_means, overall_mean):\n",
        "    num_classes = len(class_means)\n",
        "    Si = []\n",
        "    counter1 = 0\n",
        "\n",
        "    for key, value in data_dict.items():\n",
        "        for i in range(len(value)):\n",
        "            diff = value[i] - class_means[key]\n",
        "            counter1 += np.matmul(np.transpose([diff]), [diff])\n",
        "        Si.append(counter1)\n",
        "\n",
        "    SW = sum(Si)                        #within class scatter\n",
        "\n",
        "    SB = 0                              #between class scatter\n",
        "    for i in range(num_classes):\n",
        "        diffMeans = class_means[i] - overall_mean\n",
        "        SB += len(data_dict[i]) * np.matmul(np.transpose([diffMeans]), [diffMeans])\n",
        "    \n",
        "    return SB, SW\n",
        "\n",
        "def oneHot(labels):\n",
        "    num_classes = len(set(labels))\n",
        "    onehot_labels = np.zeros((len(labels), num_classes))\n",
        "    for i in range(len(labels)):          #encodes labels to 1-hot \n",
        "        #print(i, labels[i])\n",
        "        onehot_labels[i][int(labels[i])-1] = 1\n",
        "\n",
        "    return onehot_labels\n",
        "\n",
        "# projects data along W \n",
        "def projectData(W, data):\n",
        "    #print(np.shape(W))\n",
        "    #print(np.shape(data))\n",
        "    projectedData = []\n",
        "    for i in range(len(data)):\n",
        "        #projectedData.append(data[i] @ np.transpose(W))\n",
        "        projectedData.append(data[i] @ W)\n",
        "    return projectedData\n",
        "\n",
        "#finds mean, cov, and prior for each class\n",
        "def findGaussParams(data_dict, labels, W):\n",
        "    means = []\n",
        "    covs = []\n",
        "    priors = []\n",
        "    total_cov = 0\n",
        "\n",
        "    samples = len(labels)\n",
        "    for key, value in data_dict.items():\n",
        "        projection = projectData(W, value)\n",
        "        means.append(np.mean(projection, axis = 0))\n",
        "        cov = np.cov(projection, rowvar = False)      #2x2\n",
        "        covs.append(cov)\n",
        "        prior = len(data_dict[key])/samples\n",
        "        total_cov += prior * cov\n",
        "        priors.append(prior)\n",
        "        \n",
        "    return means,covs,priors, total_cov\n",
        "\n",
        "def softmax(vect):\n",
        "    e = np.exp(vect)\n",
        "    return e / e.sum()\n",
        "\n",
        "\n",
        "def plot(projdata, labels):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    ax.scatter(projdata[:,0],projdata[:,1] , c=labels, lw=0)\n",
        "\n",
        "#finds the optimal W matrix to project data to 2D\n",
        "#finds mean, cov, and prior of each class\n",
        "def trainLDA2D(train_data, labels, test_data):\n",
        "    num_classes = len(set(labels))\n",
        "    data_dict = seperateData(train_data, labels)\n",
        "    labels_onehot = oneHot(labels)\n",
        "\n",
        "    class_means, overall_mean = calcMeans(data_dict)\n",
        "    SB,SW = calcCovariances(data_dict, class_means, overall_mean)   \n",
        "    \n",
        "    eigVal, eigVect = np.linalg.eig(np.linalg.pinv(SW) @ SB)\n",
        "    eigPairs = [(abs(eigVal[i]), eigVect[:,i]) for i in range(len(eigVal))]       #combine the eigval & eigvect into pairs\n",
        "    sortedEigPairs = sorted(eigPairs, key = lambda x: x[0], reverse = True)       #sort the eigen pairs in descending order \n",
        "    largest = sortedEigPairs[0][1]\n",
        "    seclargest = sortedEigPairs[1][1]\n",
        "    W = np.transpose(np.vstack((largest,seclargest)))                              #W formed by  eigenvects corresponding to the 2 largest eigenvals\n",
        "    #print(np.shape(W))\n",
        "\n",
        "    proj = projectData(W, train_data)\n",
        "    proj = np.asarray(proj)\n",
        "    #projplot = plot(proj, labels)\n",
        "\n",
        "    proj_class_mean, proj_class_cov, priors, total_cov = findGaussParams(data_dict, labels, W)      #find mean,cov, prior for each class\n",
        "\n",
        "    return W, proj_class_mean, proj_class_cov, priors\n",
        "\n",
        "\n",
        "def predict(data, W, means, covs, priors):\n",
        "    y_hat = []\n",
        "    proj = projectData(W, data)  \n",
        "    proj = np.asarray(proj)\n",
        "    for sample in proj:\n",
        "        sample_pred = []\n",
        "        for i in range(len(means)):                     #use the un-simplified discriminant form \n",
        "            invCov = np.linalg.inv(covs[i])\n",
        "            t1 = (-1/2) * invCov\n",
        "            t2 = invCov @ means[i]\n",
        "            t3 = (-1/2) * means[i] @ invCov @ np.transpose([means[i]]) - (1/2)*np.log(np.linalg.det(covs[i])) + np.log(priors[i])\n",
        "            g = (sample @ t1 @ np.transpose([sample])) + sample @ t2 + t3\n",
        "            sample_pred.append(g)\n",
        "        pred = softmax(sample_pred)\n",
        "        y_hat.append(np.argmax(pred))\n",
        "    \n",
        "    return y_hat\n",
        "\n",
        "def LDA2dGaussGM(filename, num_crossval):\n",
        "    split_data = crossValSplit(filename)\n",
        "    \n",
        "    globalError = 0\n",
        "    globalStdDev = 0\n",
        "\n",
        "    fold_error = []\n",
        "    fold_stddev = []\n",
        "    for i in range(len(split_data)):\n",
        "        test_data = np.copy(split_data[i])\n",
        "        test_labels = test_data[:,-1]\n",
        "        test_data = test_data[:,:-1]\n",
        "        \n",
        "        train_data_folds = np.delete(np.copy(split_data),i,0)\n",
        "        folds, samples, features = np.shape(train_data_folds)\n",
        "        train_data = np.reshape(train_data_folds, (folds * samples, features))\n",
        "        train_labels = train_data[:,-1]\n",
        "        train_data = train_data[:,:-1]\n",
        "\n",
        "        W, means, covs, priors = trainLDA2D(train_data, train_labels, test_data)\n",
        "        \n",
        "        trainPred = predict(train_data, W, means, covs, priors)\n",
        "        trainError, trainStdDev = errorAndStdDev(trainPred, train_labels)\n",
        "        \n",
        "        testPred = predict(test_data, W, means, covs, priors)\n",
        "        testError, testStdDev = errorAndStdDev(testPred, test_labels)\n",
        "\n",
        "        print(\"Fold= \", i, \"  Train error= \", trainError * 100, \"%  Train std dev= \", trainStdDev, \"  Test Error\", testError * 100, \"%   Test std dev= \", testStdDev)\n",
        "        globalError += trainError \n",
        "        globalStdDev += trainStdDev\n",
        "    \n",
        "    print(\"Avg Error = \", globalError / num_crossval * 100, \"%\")\n",
        "    print(\"Avg Standard Deviation = \", globalStdDev / num_crossval)\n",
        "\n",
        "dataset = chooseDataset(\"digits\")\n",
        "LDA2dGaussGM(dataset, 10)"
      ],
      "metadata": {
        "id": "YlcH_MQmJUK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# logisticRegression "
      ],
      "metadata": {
        "id": "pewIo2Y1lzvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from random import randrange\n",
        "from collections import defaultdict\n",
        "\n",
        "#split data into 10-folds\n",
        "def crossValSplit(dataset, folds = 10):\n",
        "    fold_size = int(len(dataset) / folds)\n",
        "    split_dataset = []\n",
        "    dataset_copy = dataset\n",
        "    for i in range(folds):\n",
        "        each_fold = []\n",
        "        while len(each_fold) < fold_size:\n",
        "            randSample = randrange(len(dataset_copy[0]))\n",
        "            each_fold.append(dataset_copy[randSample])\n",
        "            np.delete(dataset_copy,randSample)\n",
        "        split_dataset.append(each_fold)\n",
        "    \n",
        "    return split_dataset \n",
        "\n",
        "def errorAndStdDev(y_hat, gndTruth):\n",
        "    counter = 0\n",
        "    temp = 0\n",
        "    gndTruthMean = sum(gndTruth) / len(gndTruth)\n",
        "    #print(\"yhat len\",len(y_hat))\n",
        "    #print(\"gnd len\",len(gndTruth))\n",
        "    for i in range(len(y_hat)):\n",
        "        #print(\"y_hat[i]\", y_hat[i], \"  gndTruth[i]\", gndTruth[i])\n",
        "        if y_hat[i] != gndTruth[i]:\n",
        "            counter += 1\n",
        "    \n",
        "    error = counter / len(y_hat)\n",
        "    \n",
        "    for i in range(len(gndTruth)):\n",
        "        temp +=  (y_hat[i] - gndTruthMean)**2\n",
        "    stdDev = np.sqrt(temp/len(gndTruth))\n",
        "    return error, stdDev\n",
        "\n",
        "def oneHot(labels, num_classes):\n",
        "    \n",
        "    onehot_labels = np.zeros((len(labels), num_classes))\n",
        "    for i in range(len(labels)):          #encodes labels to 1-hot \n",
        "        #print(i, labels[i], onehot_labels[i,:])\n",
        "        onehot_labels[i][int(labels[i])-1] = 1\n",
        "    return onehot_labels\n",
        "\n",
        "#seperate samples into classes based on their labels\n",
        "def seperateData(data, labels):\n",
        "    data_combined = [(labels[i], data[i]) for i in range(len(labels))]\n",
        "    data_dict = defaultdict(list)\n",
        "\n",
        "    for key, val in data_combined:        #seperates data based on class\n",
        "        data_dict[key].append(val)\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "def softmax(vect):\n",
        "    e = np.exp(vect)\n",
        "    return e / e.sum()\n",
        "\n",
        "def sigmoid(z):\n",
        "    ans = 1/(1 + np.exp(-z)) \n",
        "    return ans\n",
        "\n",
        "def findCost(train_labels,y_hat):\n",
        "    #total_loss = 0\n",
        "    #print(train_labels)\n",
        "    c = 1e-10\n",
        "    cost_y0 = (1 - train_labels) * np.log(1 - y_hat +c)\n",
        "    cost_y1 = train_labels * np.log(y_hat +c)\n",
        "    loss = -(1/len(y_hat)) * np.sum(cost_y0 + cost_y1)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def classify(data, W):\n",
        "    #final_pred = np.zeros(len(data))\n",
        "    #print(\"data\", np.shape(data))\n",
        "    #print(\"w\", np.shape(W))\n",
        "    pred = sigmoid(data @ W)\n",
        "\n",
        "    #final_pred = np.argmax(softmax(pred), axis = 1)\n",
        "    #final_pred = np.argmax(softmax(pred))\n",
        "    #final_pred = softmax(pred)\n",
        "    return pred\n",
        "\n",
        "def calcDeltaE(train_data, train_labels, W, num_classes, num_samples): \n",
        "\n",
        "    \n",
        "    for i in range():\n",
        "\n",
        "\n",
        "def trainBinaryLogReg(train_data, train_labels):\n",
        "    #print(\"train labels shape\", np.shape([train_labels]))\n",
        "    #print(\"data shape\", np.shape(train_data))\n",
        "    num_features = np.shape(train_data)[1]\n",
        "    num_samples = len(train_data)\n",
        "    #num_classes = len(train_labels[0])\n",
        "    W = np.random.normal(0, 1, size=(num_features,1))\n",
        "    #print(\"W shape\", np.shape(W))\n",
        "    #adsf\n",
        "    lr = 1\n",
        "    iters = 1000\n",
        "\n",
        "    error_history = []\n",
        "    for i in range(iters):\n",
        "        \n",
        "        #print(\"w shape=\", np.shape(W))\n",
        "        #print(\"data shape=\", np.shape(train_data))\n",
        "        z =  train_data @ W\n",
        "\n",
        "        y_hat = sigmoid(z)\n",
        "\n",
        "        deltaE = (np.transpose(train_data) @ (y_hat - np.transpose([train_labels])))\n",
        "        W -= lr * deltaE\n",
        "\n",
        "        error = findCost(train_labels, y_hat)\n",
        "        #print(error)\n",
        "        error_history.append(error)\n",
        "    \n",
        "    return W, error_history\n",
        "\n",
        "\n",
        "\n",
        "def trainMulticlassLogReg(train_data, train_labels):\n",
        "    num_features = np.shape(train_data)[1]\n",
        "    num_samples = len(train_data)\n",
        "    num_classes = len(train_labels[0])\n",
        "    W = np.random.normal(0, 1, size=(num_features, num_classes))\n",
        "    #print(\"W shape\", np.shape(W))\n",
        "    #adsf\n",
        "    lr = 1\n",
        "    iters = 1000\n",
        "\n",
        "    error_history = []\n",
        "    for i in range(iters):\n",
        "        z =  train_data @ W\n",
        "\n",
        "        y_hat = np.argmax(softmax(z), axis = 1)\n",
        "        \n",
        "        deltaE = calcDeltaE(train_data, train_labels, W, num_classes, num_samples)\n",
        "\n",
        "        #deltaE = (np.transpose(train_data) @ (y_hat - np.transpose([train_labels])))\n",
        "        W -= lr * deltaE\n",
        "\n",
        "        error = findCost(train_labels, y_hat)\n",
        "        #print(error)\n",
        "        error_history.append(error)\n",
        "    \n",
        "    return W, error_history\n",
        "\n",
        "def splitDataTrainTest(data):\n",
        "    data = np.asarray(data)\n",
        "    perm_data = np.random.permutation(data)\n",
        "    train_size = int(0.8 * len(data))\n",
        "    train_split = np.asarray(perm_data[:train_size])\n",
        "    test_split = np.asarray(perm_data[train_size:])\n",
        "\n",
        "    return train_split, test_split\n",
        "\n",
        "def trainPercent(train_split, percent):\n",
        "    #print(percent * len(train_split))\n",
        "    #print(percent)\n",
        "    #print(np.shape(train_split))\n",
        "    #random.shuffle(train_split)\n",
        "    train_split = np.asarray(train_split)\n",
        "    perm_train_split = np.random.permutation(train_split)\n",
        "    #train_split = np.asarray(train_split)\n",
        "    if percent != 100:\n",
        "        train_size = int(percent/100 * len(perm_train_split))\n",
        "    else:\n",
        "        return train_split\n",
        "    \n",
        "    train_data = perm_train_split[:train_size,:]\n",
        "    #print(\"train size=\", train_size, \"  len train data\", len(train_data) )\n",
        "    return train_data\n",
        "\n",
        "\n",
        "def logisticRegression(filename, dataset, num_splits, percent):\n",
        "    split_data = crossValSplit(dataset)\n",
        "    \n",
        "    globalError = 0\n",
        "    globalStdDev = 0\n",
        "\n",
        "    fold_error = []\n",
        "    fold_stddev = []\n",
        "    gen_error_history = []\n",
        "    for i in range(len(split_data)):\n",
        "        gen_error_split = []\n",
        "        num_samples = len(split_data[i])\n",
        "\n",
        "        train_split, test_split = splitDataTrainTest(split_data[i])\n",
        "        #print(\"train split len\", len(train_split))\n",
        "        test_labels = test_split[:,-1]\n",
        "        num_classes = len(set(test_labels))\n",
        "        \n",
        "        onehot_test_labels = oneHot(test_labels, num_classes)\n",
        "        test_data = test_split[:,:-1]\n",
        "\n",
        "        for j in percent:\n",
        "\n",
        "            train_percent = trainPercent(train_split, j)\n",
        "            train_labels = train_percent[:,-1]\n",
        "            \n",
        "            train_data = train_percent[:,:-1]\n",
        "\n",
        "            if num_classes <=2:\n",
        "                W, cost_history = trainBinaryLogReg(train_data, train_labels)\n",
        "\n",
        "            else:\n",
        "                onehot_train_labels = oneHot(train_labels, num_classes)\n",
        "                W, cost_history = trainMulticlassLogReg(train_data, onehot_train_labels)\n",
        "\n",
        "\n",
        "            trainPred = classify(train_data, W)\n",
        "            #print(trainPred)\n",
        "            trainError, trainStdDev = errorAndStdDev(trainPred, train_labels)\n",
        "            #print(trainError)\n",
        "            testPred = classify(test_data,W)\n",
        "            testError, testStdDev = errorAndStdDev(testPred, test_labels)\n",
        "            #print(\"percent=\",j,\"train error=\", trainError * 100,\"  test error=\", testError * 100)\n",
        "            #trainPred = predict(train_data,W,u,sigma,priors)\n",
        "            #trainError, trainStdDev = errorAndStdDev(trainPred, train_data_labels)\n",
        "            gen_error_split.append(testError)\n",
        "        #print(\"   \")\n",
        "        gen_error_history.append(gen_error_split)\n",
        "\n",
        "    avg_error = np.mean(gen_error_history, axis = 0)\n",
        "    error_stddev = np.std(gen_error_history, axis = 0)\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.errorbar(percent, avg_error ,error_stddev,fmt='-o')\n",
        "    #ax.plot(percent,avg_error , c=labels, lw=0)\n",
        "\n",
        "percent = [10,25,50,75,100]\n",
        "#datasetNames = [\"Boston50\", \"Boston75\", \"digits\"]\n",
        "datasetNames = [\"Boston50\", \"Boston75\"]   #[\"Boston50\"]\n",
        "for i in datasetNames:\n",
        "    dataset = chooseDataset(i)\n",
        "    logisticRegression(i, dataset, 10, percent)"
      ],
      "metadata": {
        "id": "NmK3ecGX8hff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "85344072-e5c7-4af6-9b4a-49013487e15c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:61: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# naiveBayesGaussian"
      ],
      "metadata": {
        "id": "7SUq0SsqdFjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from random import randrange\n",
        "from collections import defaultdict\n",
        "\n",
        "#split data into 10-folds\n",
        "def crossValSplit(dataset, folds = 10):\n",
        "    fold_size = int(len(dataset) / folds)\n",
        "    split_dataset = []\n",
        "    dataset_copy = dataset\n",
        "    for i in range(folds):\n",
        "        each_fold = []\n",
        "        while len(each_fold) < fold_size:\n",
        "            randSample = randrange(len(dataset_copy[0]))\n",
        "            each_fold.append(dataset_copy[randSample])\n",
        "            np.delete(dataset_copy,randSample)\n",
        "        split_dataset.append(each_fold)\n",
        "    \n",
        "    return split_dataset \n",
        "\n",
        "def errorAndStdDev(y_hat, gndTruth):\n",
        "    counter = 0\n",
        "    temp = 0\n",
        "    gndTruthMean = sum(gndTruth) / len(gndTruth)\n",
        "    #print(\"yhat len\",len(y_hat))\n",
        "    #print(\"gnd len\",len(gndTruth))\n",
        "    for i in range(len(y_hat)):\n",
        "        #print(\"y_hat[i]\", y_hat[i], \"  gndTruth[i]\", gndTruth[i])\n",
        "        if y_hat[i] != gndTruth[i]:\n",
        "            counter += 1\n",
        "    \n",
        "    error = counter / len(y_hat)\n",
        "    \n",
        "    for i in range(len(gndTruth)):\n",
        "        temp +=  (y_hat[i] - gndTruthMean)**2\n",
        "    stdDev = np.sqrt(temp/len(gndTruth))\n",
        "    return error, stdDev\n",
        "\n",
        "#seperate samples into classes based on their labels\n",
        "def seperateData(data, labels):\n",
        "    data_combined = [(labels[i], data[i]) for i in range(len(labels))]\n",
        "    data_dict = defaultdict(list)\n",
        "\n",
        "    for key, val in data_combined:        #seperates data based on class\n",
        "        data_dict[key].append(val)\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "#find mean, std dev, prior for training data which will be used to later to find the likelihood\n",
        "def calcParams(data_dict, num_classes, num_features):\n",
        "    class_means = np.zeros((num_classes,num_features))\n",
        "    class_stds = np.zeros((num_classes,num_features))\n",
        "    priors = np.zeros((num_classes,1))\n",
        "\n",
        "    for key,val in data_dict.items():\n",
        "        class_means[int(key),:] = np.mean(val, axis = 0)\n",
        "        class_stds[int(key),:] = np.std(val, axis = 0)\n",
        "        priors[int(key)] = len(val)\n",
        "\n",
        "    smallvalmean = 1e-10 * np.ones((np.shape(class_means)))       #add some small values to avoid dividing by 0 and other complications\n",
        "    smallvalstd = 1e-10 * np.ones((np.shape(class_stds)))\n",
        "    class_means += smallvalmean\n",
        "    class_stds += smallvalstd\n",
        "\n",
        "    return class_means, class_stds, priors\n",
        "\n",
        "\n",
        "#finds likelihood of sample using mean & std dev found in \"training\"\n",
        "def calcLikelihood(sample,mean, std, num_classes):\n",
        "    likelihood = np.sum(np.divide((sample - mean), std) ** 2)\n",
        "    #print(likelihood)\n",
        "    return likelihood \n",
        "\n",
        "#classify sample using naive bayes by multiplying the likelihood and ln(prior) \n",
        "def classify(sample,class_means, class_stds, priors):\n",
        "    pred = []\n",
        "    num_classes = len(class_means)\n",
        "    for c in range(num_classes):                #find the probability for each class \n",
        "        likelihood = calcLikelihood(sample,class_means[c], class_stds[c], num_classes)\n",
        "        g = (-1/2) * likelihood + np.log(priors[c])\n",
        "        pred.append(g)\n",
        "    y_hat = np.argmax(pred)                     #predicted value is the index of the max probability \n",
        "    return y_hat \n",
        "\n",
        "\n",
        "#generate class predictions\n",
        "def predict(data, class_means, class_stds, priors):\n",
        "    y_hat = []\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        prediction = classify(data[i], class_means, class_stds, priors)\n",
        "        y_hat.append(prediction)\n",
        "    return y_hat \n",
        "\n",
        "#finds parameters needed for naive bayes \n",
        "def trainNaiveBayes(train_data, train_labels, num_classes):\n",
        "    num_features = len(train_data[0])\n",
        "    data_dict = seperateData(train_data, train_labels)\n",
        "    class_means, class_stds, priors = calcParams(data_dict, num_classes, num_features)\n",
        "    priors = priors / len(train_data)\n",
        "    return class_means, class_stds, priors\n",
        "\n",
        "#split total data into train(80%) and test(20%) sets\n",
        "def splitDataTrainTest(data):             \n",
        "    data = np.asarray(data)\n",
        "    perm_data = np.random.permutation(data)\n",
        "    train_size = int(0.8 * len(data))\n",
        "    train_split = np.asarray(perm_data[:train_size])\n",
        "    test_split = np.asarray(perm_data[train_size:])\n",
        "\n",
        "    return train_split, test_split\n",
        "\n",
        "#partition training set to be [10%,25%,50%,75%,100%] of the total training set size\n",
        "def trainPercent(train_split, percent):         \n",
        "    train_split = np.asarray(train_split)\n",
        "    perm_train_split = np.random.permutation(train_split)\n",
        "    if percent != 100:\n",
        "        train_size = int(percent/100 * len(perm_train_split))\n",
        "    else:\n",
        "        return train_split \n",
        "    train_data = perm_train_split[:train_size,:]\n",
        "    return train_data\n",
        "\n",
        "\n",
        "def naiveBayesGaussian(filename, dataset, num_splits, percent):\n",
        "    split_data = crossValSplit(dataset)\n",
        "    \n",
        "    globalError = 0\n",
        "    globalStdDev = 0\n",
        "    \n",
        "    fold_error = []\n",
        "    fold_stddev = []\n",
        "    gen_error_history = []\n",
        "    for i in range(len(split_data)):\n",
        "        gen_error_split = []\n",
        "        num_samples = len(split_data[i])\n",
        "\n",
        "        train_split, test_split = splitDataTrainTest(split_data[i])\n",
        "        test_labels = test_split[:,-1]\n",
        "        num_classes = len(set(test_labels))\n",
        "        if num_classes > 2:\n",
        "            num_classes = 10\n",
        "        else:\n",
        "            num_classes = 2\n",
        "\n",
        "        test_data = test_split[:,:-1]\n",
        "\n",
        "        for j in percent:\n",
        "            train_percent = trainPercent(train_split, j)\n",
        "            train_labels = train_percent[:,-1]\n",
        "            train_data = train_percent[:,:-1]\n",
        "\n",
        "            class_means, class_stds, priors = trainNaiveBayes(train_data, train_labels, num_classes)\n",
        "\n",
        "            trainPred = predict(train_data, class_means, class_stds, priors)\n",
        "            trainError, trainStdDev = errorAndStdDev(trainPred, train_labels)\n",
        "            \n",
        "            testPred = predict(test_data,class_means, class_stds, priors)\n",
        "            testError, testStdDev = errorAndStdDev(testPred, test_labels)\n",
        "            print(\"percent=\",j,\"train error=\", trainError * 100,\"  test error=\", testError * 100)\n",
        "\n",
        "            gen_error_split.append(testError)\n",
        "        \n",
        "        gen_error_history.append(gen_error_split)\n",
        "\n",
        "    avg_error = np.mean(gen_error_history, axis = 0)\n",
        "    error_stddev = np.std(gen_error_history, axis = 0)\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.errorbar(percent, avg_error ,error_stddev,fmt='-o')\n",
        "\n",
        "\n",
        "percent = [10,25,50,75,100]\n",
        "#datasetNames = [\"digits\"]\n",
        "datasetNames = [\"Boston50\", \"Boston75\", \"digits\"]   #[\"Boston50\"]\n",
        "for i in datasetNames:\n",
        "    dataset = chooseDataset(i)\n",
        "    naiveBayesGaussian(i, dataset, 10, percent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tU_TGgmjdNvK",
        "outputId": "3e8b0655-e622-4a6e-b18c-be91fae5d8a4"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "percent= 10 train error= 0.0   test error= 40.0\n",
            "percent= 25 train error= 10.0   test error= 10.0\n",
            "percent= 50 train error= 10.0   test error= 10.0\n",
            "percent= 75 train error= 16.666666666666664   test error= 10.0\n",
            "percent= 100 train error= 17.5   test error= 10.0\n",
            "percent= 10 train error= 0.0   test error= 50.0\n",
            "percent= 25 train error= 0.0   test error= 0.0\n",
            "percent= 50 train error= 20.0   test error= 10.0\n",
            "percent= 75 train error= 6.666666666666667   test error= 0.0\n",
            "percent= 100 train error= 5.0   test error= 0.0\n",
            "percent= 10 train error= 0.0   test error= 80.0\n",
            "percent= 25 train error= 10.0   test error= 0.0\n",
            "percent= 50 train error= 10.0   test error= 0.0\n",
            "percent= 75 train error= 10.0   test error= 0.0\n",
            "percent= 100 train error= 10.0   test error= 0.0\n",
            "percent= 10 train error= 0.0   test error= 50.0\n",
            "percent= 25 train error= 10.0   test error= 10.0\n",
            "percent= 50 train error= 20.0   test error= 20.0\n",
            "percent= 75 train error= 20.0   test error= 20.0\n",
            "percent= 100 train error= 17.5   test error= 20.0\n",
            "percent= 10 train error= 0.0   test error= 60.0\n",
            "percent= 25 train error= 10.0   test error= 0.0\n",
            "percent= 50 train error= 10.0   test error= 10.0\n",
            "percent= 75 train error= 6.666666666666667   test error= 0.0\n",
            "percent= 100 train error= 15.0   test error= 10.0\n",
            "percent= 10 train error= 0.0   test error= 30.0\n",
            "percent= 25 train error= 10.0   test error= 20.0\n",
            "percent= 50 train error= 10.0   test error= 20.0\n",
            "percent= 75 train error= 16.666666666666664   test error= 30.0\n",
            "percent= 100 train error= 12.5   test error= 20.0\n",
            "percent= 10 train error= 0.0   test error= 20.0\n",
            "percent= 25 train error= 10.0   test error= 10.0\n",
            "percent= 50 train error= 5.0   test error= 30.0\n",
            "percent= 75 train error= 20.0   test error= 30.0\n",
            "percent= 100 train error= 22.5   test error= 30.0\n",
            "percent= 10 train error= 0.0   test error= 70.0\n",
            "percent= 25 train error= 0.0   test error= 20.0\n",
            "percent= 50 train error= 5.0   test error= 10.0\n",
            "percent= 75 train error= 13.333333333333334   test error= 20.0\n",
            "percent= 100 train error= 10.0   test error= 20.0\n",
            "percent= 10 train error= 0.0   test error= 60.0"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:79: RuntimeWarning: divide by zero encountered in log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "percent= 25 train error= 0.0   test error= 0.0\n",
            "percent= 50 train error= 15.0   test error= 20.0\n",
            "percent= 75 train error= 13.333333333333334   test error= 0.0\n",
            "percent= 100 train error= 10.0   test error= 0.0\n",
            "percent= 10 train error= 0.0   test error= 30.0\n",
            "percent= 25 train error= 0.0   test error= 0.0\n",
            "percent= 50 train error= 15.0   test error= 20.0\n",
            "percent= 75 train error= 20.0   test error= 20.0\n",
            "percent= 100 train error= 20.0   test error= 20.0\n",
            "percent= 10 train error= 0.0   test error= 30.0\n",
            "percent= 25 train error= 0.0   test error= 20.0\n",
            "percent= 50 train error= 0.0   test error= 20.0\n",
            "percent= 75 train error= 3.3333333333333335   test error= 20.0\n",
            "percent= 100 train error= 5.0   test error= 20.0\n",
            "percent= 10 train error= 0.0   test error= 60.0\n",
            "percent= 25 train error= 10.0   test error= 0.0\n",
            "percent= 50 train error= 40.0   test error= 30.0\n",
            "percent= 75 train error= 20.0   test error= 0.0\n",
            "percent= 100 train error= 15.0   test error= 0.0\n",
            "percent= 10 train error= 0.0   test error= 40.0\n",
            "percent= 25 train error= 0.0   test error= 0.0\n",
            "percent= 50 train error= 35.0   test error= 30.0\n",
            "percent= 75 train error= 13.333333333333334   test error= 30.0\n",
            "percent= 100 train error= 22.5   test error= 30.0\n",
            "percent= 10 train error= 0.0   test error= 30.0\n",
            "percent= 25 train error= 20.0   test error= 10.0\n",
            "percent= 50 train error= 15.0   test error= 10.0\n",
            "percent= 75 train error= 26.666666666666668   test error= 10.0\n",
            "percent= 100 train error= 25.0   test error= 10.0\n",
            "percent= 10 train error= 25.0   test error= 50.0\n",
            "percent= 25 train error= 10.0   test error= 30.0\n",
            "percent= 50 train error= 10.0   test error= 30.0\n",
            "percent= 75 train error= 10.0   test error= 30.0\n",
            "percent= 100 train error= 15.0   test error= 30.0\n",
            "percent= 10 train error= 25.0   test error= 10.0\n",
            "percent= 25 train error= 0.0   test error= 10.0\n",
            "percent= 50 train error= 0.0   test error= 0.0\n",
            "percent= 75 train error= 0.0   test error= 0.0\n",
            "percent= 100 train error= 12.5   test error= 0.0\n",
            "percent= 10 train error= 0.0   test error= 50.0\n",
            "percent= 25 train error= 0.0   test error= 40.0\n",
            "percent= 50 train error= 25.0   test error= 10.0\n",
            "percent= 75 train error= 16.666666666666664   test error= 10.0\n",
            "percent= 100 train error= 20.0   test error= 10.0\n",
            "percent= 10 train error= 0.0   test error= 10.0\n",
            "percent= 25 train error= 0.0   test error= 10.0\n",
            "percent= 50 train error= 0.0   test error= 0.0\n",
            "percent= 75 train error= 0.0   test error= 0.0\n",
            "percent= 100 train error= 2.5   test error= 0.0\n",
            "percent= 10 train error= 25.0   test error= 60.0\n",
            "percent= 25 train error= 10.0   test error= 20.0\n",
            "percent= 50 train error= 0.0   test error= 20.0\n",
            "percent= 75 train error= 16.666666666666664   test error= 20.0\n",
            "percent= 100 train error= 12.5   test error= 20.0\n",
            "percent= 10 train error= 0.0   test error= 40.0\n",
            "percent= 25 train error= 0.0   test error= 10.0\n",
            "percent= 50 train error= 15.0   test error= 10.0\n",
            "percent= 75 train error= 10.0   test error= 10.0\n",
            "percent= 100 train error= 20.0   test error= 10.0\n",
            "percent= 10 train error= 0.0   test error= 83.33333333333334\n",
            "percent= 25 train error= 0.0   test error= 27.77777777777778\n",
            "percent= 50 train error= 0.0   test error= 13.88888888888889\n",
            "percent= 75 train error= 0.0   test error= 0.0\n",
            "percent= 100 train error= 0.0   test error= 0.0\n",
            "percent= 10 train error= 0.0   test error= 61.111111111111114\n",
            "percent= 25 train error= 0.0   test error= 36.11111111111111\n",
            "percent= 50 train error= 0.0   test error= 16.666666666666664\n",
            "percent= 75 train error= 0.0   test error= 2.7777777777777777\n",
            "percent= 100 train error= 0.0   test error= 2.7777777777777777\n",
            "percent= 10 train error= 0.0   test error= 50.0\n",
            "percent= 25 train error= 0.0   test error= 16.666666666666664\n",
            "percent= 50 train error= 0.0   test error= 13.88888888888889\n",
            "percent= 75 train error= 0.0   test error= 5.555555555555555\n",
            "percent= 100 train error= 0.0   test error= 2.7777777777777777\n",
            "percent= 10 train error= 0.0   test error= 55.55555555555556\n",
            "percent= 25 train error= 0.0   test error= 13.88888888888889\n",
            "percent= 50 train error= 0.0   test error= 13.88888888888889\n",
            "percent= 75 train error= 0.0   test error= 2.7777777777777777\n",
            "percent= 100 train error= 0.0   test error= 2.7777777777777777\n",
            "percent= 10 train error= 0.0   test error= 58.333333333333336\n",
            "percent= 25 train error= 0.0   test error= 25.0\n",
            "percent= 50 train error= 0.0   test error= 5.555555555555555\n",
            "percent= 75 train error= 0.0   test error= 2.7777777777777777\n",
            "percent= 100 train error= 0.0   test error= 2.7777777777777777\n",
            "percent= 10 train error= 0.0   test error= 30.555555555555557\n",
            "percent= 25 train error= 0.0   test error= 19.444444444444446\n",
            "percent= 50 train error= 0.0   test error= 0.0\n",
            "percent= 75 train error= 0.0   test error= 0.0\n",
            "percent= 100 train error= 0.0   test error= 0.0\n",
            "percent= 10 train error= 0.0   test error= 61.111111111111114\n",
            "percent= 25 train error= 0.0   test error= 13.88888888888889\n",
            "percent= 50 train error= 0.0   test error= 5.555555555555555\n",
            "percent= 75 train error= 0.0   test error= 0.0\n",
            "percent= 100 train error= 0.0   test error= 0.0\n",
            "percent= 10 train error= 0.0   test error= 61.111111111111114\n",
            "percent= 25 train error= 0.0   test error= 27.77777777777778\n",
            "percent= 50 train error= 0.0   test error= 16.666666666666664\n",
            "percent= 75 train error= 0.0   test error= 8.333333333333332\n",
            "percent= 100 train error= 0.0   test error= 8.333333333333332\n",
            "percent= 10 train error= 0.0   test error= 50.0\n",
            "percent= 25 train error= 0.0   test error= 36.11111111111111\n",
            "percent= 50 train error= 0.0   test error= 16.666666666666664\n",
            "percent= 75 train error= 0.0   test error= 16.666666666666664\n",
            "percent= 100 train error= 0.0   test error= 16.666666666666664\n",
            "percent= 10 train error= 0.0   test error= 44.44444444444444\n",
            "percent= 25 train error= 0.0   test error= 38.88888888888889\n",
            "percent= 50 train error= 0.0   test error= 11.11111111111111\n",
            "percent= 75 train error= 0.0   test error= 0.0\n",
            "percent= 100 train error= 0.0   test error= 0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}